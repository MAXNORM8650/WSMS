{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "577fc625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n",
    "import random\n",
    "from utils.read_dataset import read_dataset\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5850e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import num_classes, model_name, model_path, lr_milestones, lr_decay_rate, input_size, \\\n",
    "    root, end_epoch, save_interval, init_lr, batch_size, CUDA_VISIBLE_DEVICES, weight_decay, \\\n",
    "    proposalN, set, channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25653b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef4611bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pop=r\"C:\\Users\\Admin\\Dropbox\\PC\\Documents\\FGVC_MSFM\\MMAL-Net\\datasets\\multiclass_path\\train_positive.txt\"\n",
    "# nop=r\"C:\\Users\\Admin\\Dropbox\\PC\\Documents\\FGVC_MSFM\\MMAL-Net\\datasets\\multiclass_path\\train_negative.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa218e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf=pd.read_table(nop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "489c0ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf=pdf.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c47c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf.to_csv(nop, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d317368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1f012ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_pretrained_vit import ViT\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n",
    "           'wide_resnet50_2', 'wide_resnet101_2']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
    "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
    "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
    "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, return_cam=True, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.num_classes=num_classes\n",
    "        self.base_width = width_per_group\n",
    "        self.return_cam=return_cam\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        # self.conv2 = nn.Conv2d(in_channels=2048, out_channels=2048, kernel_size=4, stride=1,bias=False)\n",
    "        # self.conv3 = nn.Conv2d(in_channels=2048, out_channels=201, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(512 * block.expansion, self.num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x=ViT('B_16')(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        conv5_b = self.layer4[:2](x)\n",
    "        x = self.layer4[2](conv5_b)\n",
    "\n",
    "        fm = x\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "       \n",
    "        embeeding = x\n",
    "        \n",
    "        return fm, embeeding\n",
    "\n",
    "\n",
    "def _resnet(arch, block, layers, pretrained, pth_path, **kwargs):\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = torch.load(pth_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet18(pth_path, pretrained=False, **kwargs):\n",
    "    r\"\"\"ResNet-18 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, pth_path,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def resnet34(pth_path, pretrained=False, **kwargs):\n",
    "    r\"\"\"ResNet-34 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, pth_path,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def resnet50(pth_path, pretrained=False, **kwargs):\n",
    "    r\"\"\"ResNet-50 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3],pretrained, pth_path,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def resnet101(pth_path, pretrained=False, **kwargs):\n",
    "    r\"\"\"ResNet-101 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], pretrained, pth_path,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def resnet152(pth_path, pretrained=False, **kwargs):\n",
    "    r\"\"\"ResNet-152 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, pth_path,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def resnext50_32x4d(pth_path, pretrained=False, **kwargs):\n",
    "    r\"\"\"ResNeXt-50 32x4d model from\n",
    "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 4\n",
    "    return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3],\n",
    "                   pretrained, pth_path, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def resnext101_32x8d(pth_path, pretrained=False, **kwargs):\n",
    "    r\"\"\"ResNeXt-101 32x8d model from\n",
    "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 8\n",
    "    return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3],\n",
    "                   pretrained, pth_path, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def wide_resnet50_2(pth_path, pretrained=False, **kwargs):\n",
    "    r\"\"\"Wide ResNet-50-2 model from\n",
    "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n",
    "\n",
    "    The model is the same as ResNet except for the bottleneck number of channels\n",
    "    which is twice larger in every block. The number of channels in outer 1x1\n",
    "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
    "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['width_per_group'] = 64 * 2\n",
    "    return _resnet('wide_resnet50_2', Bottleneck, [3, 4, 6, 3],\n",
    "                   pretrained, pth_path, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def wide_resnet101_2(pth_path, pretrained=False, **kwargs):\n",
    "    r\"\"\"Wide ResNet-101-2 model from\n",
    "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n",
    "\n",
    "    The model is the same as ResNet except for the bottleneck number of channels\n",
    "    which is twice larger in every block. The number of channels in outer 1x1\n",
    "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
    "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['width_per_group'] = 64 * 2\n",
    "    return _resnet('wide_resnet101_2', Bottleneck, [3, 4, 23, 3],\n",
    "                   pretrained, pth_path, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15364611",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=torch.rand(1, 3, 448, 448)\n",
    "pretrain_path_B='./models/pretrained/resnet_mura_pr.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560ea594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4a0d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4315c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3906f85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from skimage import measure\n",
    "def AOLM(fms, fm1):\n",
    "    A = torch.sum(fms, dim=1, keepdim=True)\n",
    "    a = torch.mean(A, dim=[2, 3], keepdim=True)\n",
    "    M = (A > a).float()\n",
    "\n",
    "    A1 = torch.sum(fm1, dim=1, keepdim=True)\n",
    "    a1 = torch.mean(A1, dim=[2, 3], keepdim=True)\n",
    "    M1 = (A1 > a1).float()\n",
    "\n",
    "\n",
    "    coordinates = []\n",
    "    for i, m in enumerate(M):\n",
    "        mask_np = m.cpu().numpy().reshape(14, 14)\n",
    "        component_labels = measure.label(mask_np)\n",
    "\n",
    "        properties = measure.regionprops(component_labels)\n",
    "        areas = []\n",
    "        for prop in properties:\n",
    "            areas.append(prop.area)\n",
    "        max_idx = areas.index(max(areas))\n",
    "\n",
    "\n",
    "        intersection = ((component_labels==(max_idx+1)).astype(int) + (M1[i][0].cpu().numpy()==1).astype(int)) ==2\n",
    "        prop = measure.regionprops(intersection.astype(int))\n",
    "        if len(prop) == 0:\n",
    "            bbox = [0, 0, 14, 14]\n",
    "#             print('there is one img no intersection')\n",
    "        else:\n",
    "            bbox = prop[0].bbox\n",
    "#         print(\"B box is\",bbox)\n",
    "        x_lefttop = bbox[0] * 32 - 1\n",
    "        y_lefttop = bbox[1] * 32 - 1\n",
    "        x_rightlow = bbox[2] * 32 - 1\n",
    "        y_rightlow = bbox[3] * 32 - 1\n",
    "        # for image\n",
    "        if x_lefttop < 0:\n",
    "            x_lefttop = 0\n",
    "        if y_lefttop < 0:\n",
    "            y_lefttop = 0\n",
    "        coordinate = [x_lefttop, y_lefttop, x_rightlow, y_rightlow]\n",
    "        coordinates.append(coordinate)\n",
    "    return coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2689195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "##################################\n",
    "# augment function\n",
    "##################################\n",
    "def batch_augment(images, attention_map, mode='drop', theta=0.4, padding_ratio=0.1, red_p=0.9):\n",
    "    batches, _, imgH, imgW = images.size()\n",
    "\n",
    "    if mode == 'crop':\n",
    "        crop_images = []\n",
    "        for batch_index in range(batches):\n",
    "            atten_map = attention_map[batch_index:batch_index + 1]\n",
    "            if isinstance(theta, tuple):\n",
    "                theta_c = random.uniform(*theta) * atten_map.max()\n",
    "            else:\n",
    "                theta_c = theta * atten_map.max()\n",
    "\n",
    "            crop_mask = F.upsample_bilinear(atten_map, size=(imgH, imgW)) >= theta_c\n",
    "            nonzero_indices = torch.nonzero(crop_mask[0, 0, ...])\n",
    "            height_min = max(int(nonzero_indices[:, 0].min().item() - padding_ratio * imgH), 0)\n",
    "            height_max = min(int(nonzero_indices[:, 0].max().item() + padding_ratio * imgH), imgH)\n",
    "            width_min = max(int(nonzero_indices[:, 1].min().item() - padding_ratio * imgW), 0)\n",
    "            width_max = min(int(nonzero_indices[:, 1].max().item() + padding_ratio * imgW), imgW)\n",
    "\n",
    "            crop_images.append(\n",
    "                F.upsample_bilinear(images[batch_index:batch_index + 1, :, height_min:height_max, width_min:width_max],\n",
    "                                    size=(imgH, imgW)))\n",
    "        crop_images = torch.cat(crop_images, dim=0)\n",
    "        return crop_images\n",
    "\n",
    "    elif mode == 'drop':\n",
    "        drop_masks = []\n",
    "        for batch_index in range(batches):\n",
    "            atten_map = attention_map[batch_index:batch_index + 1]\n",
    "            if isinstance(theta, tuple):\n",
    "                theta_d = random.uniform(*theta) * atten_map.max()\n",
    "            else:\n",
    "                theta_d = theta * atten_map.max()\n",
    "            msk=F.upsample_bilinear(atten_map, size=(imgH, imgW)) < theta_d\n",
    "            q =msk.float()\n",
    "            tense_tensor = torch.tensor(q, device='cuda')\n",
    "            drop_masks.append(torch.where(tense_tensor==0.0, torch.tensor(red_p, device = 'cuda'), tense_tensor))\n",
    "        drop_masks = torch.cat(drop_masks, dim=0)\n",
    "        drop_images = images * drop_masks\n",
    "        return drop_images\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Expected mode in [\\'crop\\', \\'drop\\'], but received unsupported augmentation method %s' % mode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13fe335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_B = resnet50(pretrained=False, pth_path=pretrain_path_B)\n",
    "# ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e6abf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from networks import Axial_Nets\n",
    "from networks import resnet\n",
    "from config import pretrain_path, coordinates_cat, iou_threshs, window_nums_sum, ratios, N_list\n",
    "import numpy as np\n",
    "# from utils.AOLM import AOLM\n",
    "pretrain_path_B = './models/pretrained/resnet50-19c8e357.pth'\n",
    "# PATH = './models/pretrained/resnet_mura_pr.pth'\n",
    "def nms(scores_np, proposalN, iou_threshs, coordinates):\n",
    "    if not (type(scores_np).__module__ == 'numpy' and len(scores_np.shape) == 2 and scores_np.shape[1] == 1):\n",
    "        raise TypeError('score_np is not right')\n",
    "\n",
    "    windows_num = scores_np.shape[0]\n",
    "    indices_coordinates = np.concatenate((scores_np, coordinates), 1)\n",
    "\n",
    "    indices = np.argsort(indices_coordinates[:, 0])\n",
    "    indices_coordinates = np.concatenate((indices_coordinates, np.arange(0,windows_num).reshape(windows_num,1)), 1)[indices]                  #[339,6]\n",
    "    indices_results = []\n",
    "\n",
    "    res = indices_coordinates\n",
    "\n",
    "    while res.any():\n",
    "        indice_coordinates = res[-1]\n",
    "        indices_results.append(indice_coordinates[5])\n",
    "\n",
    "        if len(indices_results) == proposalN:\n",
    "            return np.array(indices_results).reshape(1,proposalN).astype(np.int64)\n",
    "        res = res[:-1]\n",
    "\n",
    "        # Exclude anchor boxes with selected anchor box whose iou is greater than the threshold\n",
    "        start_max = np.maximum(res[:, 1:3], indice_coordinates[1:3])\n",
    "        end_min = np.minimum(res[:, 3:5], indice_coordinates[3:5])\n",
    "        lengths = end_min - start_max + 1\n",
    "        intersec_map = lengths[:, 0] * lengths[:, 1]\n",
    "        intersec_map[np.logical_or(lengths[:, 0] < 0, lengths[:, 1] < 0)] = 0\n",
    "        iou_map_cur = intersec_map / ((res[:, 3] - res[:, 1] + 1) * (res[:, 4] - res[:, 2] + 1) +\n",
    "                                      (indice_coordinates[3] - indice_coordinates[1] + 1) *\n",
    "                                      (indice_coordinates[4] - indice_coordinates[2] + 1) - intersec_map)\n",
    "        res = res[iou_map_cur <= iou_threshs]\n",
    "\n",
    "    while len(indices_results) != proposalN:\n",
    "        indices_results.append(indice_coordinates[5])\n",
    "\n",
    "    return np.array(indices_results).reshape(1, -1).astype(np.int64)\n",
    "\n",
    "class APPM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(APPM, self).__init__()\n",
    "        self.avgpools = [nn.AvgPool2d(ratios[i], 1) for i in range(len(ratios))]\n",
    "\n",
    "    def forward(self, proposalN, x, ratios, window_nums_sum, N_list, iou_threshs, DEVICE='cuda'):\n",
    "        batch, channels, _, _ = x.size()\n",
    "        avgs = [self.avgpools[i](x) for i in range(len(ratios))]\n",
    "\n",
    "        # feature map sum\n",
    "        fm_sum = [torch.sum(avgs[i], dim=1) for i in range(len(ratios))]\n",
    "\n",
    "        all_scores = torch.cat([fm_sum[i].view(batch, -1, 1) for i in range(len(ratios))], dim=1)\n",
    "        windows_scores_np = all_scores.data.cpu().numpy()\n",
    "        window_scores = torch.from_numpy(windows_scores_np).to(DEVICE).reshape(batch, -1)\n",
    "\n",
    "        # nms\n",
    "        proposalN_indices = []\n",
    "        for i, scores in enumerate(windows_scores_np):\n",
    "            indices_results = []\n",
    "            for j in range(len(window_nums_sum)-1):\n",
    "                indices_results.append(nms(scores[sum(window_nums_sum[:j+1]):sum(window_nums_sum[:j+2])], proposalN=N_list[j], iou_threshs=iou_threshs[j],\n",
    "                                           coordinates=coordinates_cat[sum(window_nums_sum[:j+1]):sum(window_nums_sum[:j+2])]) + sum(window_nums_sum[:j+1]))\n",
    "            # indices_results.reverse()\n",
    "            proposalN_indices.append(np.concatenate(indices_results, 1))   # reverse\n",
    "\n",
    "        proposalN_indices = np.array(proposalN_indices).reshape(batch, proposalN)\n",
    "        proposalN_indices = torch.from_numpy(proposalN_indices).to(DEVICE)\n",
    "        proposalN_windows_scores = torch.cat(\n",
    "            [torch.index_select(all_score, dim=0, index=proposalN_indices[i]) for i, all_score in enumerate(all_scores)], 0).reshape(\n",
    "            batch, proposalN)\n",
    "\n",
    "        return proposalN_indices, proposalN_windows_scores, window_scores\n",
    "\n",
    "class MainNet(nn.Module):\n",
    "    def __init__(self, proposalN, num_classes, channels, theta, red_p):\n",
    "       \n",
    "        super(MainNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.proposalN = proposalN\n",
    "        self.M=3\n",
    "        self.theta=theta\n",
    "        self.red_p = red_p\n",
    "#         self.pretrained_model =Net()\n",
    "#         self.pretrained_model = axial50s(k=56)\n",
    "\n",
    "        self.pretrained_model_A = resnet.resnet50(pretrained=True, pth_path=pretrain_path)\n",
    "        self.pretrained_model_B = resnet50(pretrained=True, pth_path=pretrain_path_B)\n",
    "        self.rawcls_net = nn.Linear(channels, num_classes)\n",
    "        self.APPM = APPM()\n",
    "# (self, layers, output_dim, heads, input_resolution=448, width=64)\n",
    "    def forward(self, x, epoch, batch_idx, status='test', DEVICE='cuda'):\n",
    "        att_out = x\n",
    "#         att_out=batch_augment(x, torch.sum(self.pretrained_model_A(x)[-1], dim=1, keepdim=True),\n",
    "#                               mode='drop', theta=self.theta, red_p=self.red_p)\n",
    "        \n",
    "        fm, embedding = self.pretrained_model_B(x)\n",
    "        raw_logits = self.rawcls_net(embedding)\n",
    "        feature_map = fm.detach().clone()\n",
    "        fc_weights = self.rawcls_net.weight.view(\n",
    "            1, self.num_classes, feature_map.shape[1], 1, 1)  # 1 * L * C * 1 * 1\n",
    "        feature = feature_map.unsqueeze(1)  # N * 1 * C * H * W\n",
    "        CFM = (feature * fc_weights).sum(2)  # N * L * H * W\n",
    "#         embedding=self.pretrained_model2(embedding)\n",
    "        batch_size, channel_size, side_size, _ = fm.shape\n",
    "        assert channel_size == 2048\n",
    "\n",
    "        # raw branch\n",
    "        \n",
    "#         print(\"raw_logits\",raw_logits)\n",
    "#         y=raw_logits.argmax(dim=-1)\n",
    "\n",
    "        #SCDA\n",
    "#         coordinats=torch.tensor(TSCM(tscams,y,x,batch_size))\n",
    "        coordinates = torch.tensor(AOLM(fm.detach(), CFM.detach()))\n",
    "#         print(\"coordinate\",coordinates)\n",
    "\n",
    "        local_imgs = torch.zeros([batch_size, 3, 448, 448]).to(DEVICE)  # [N, 3, 448, 448]\n",
    "        for i in range(batch_size):\n",
    "            [x0, y0, x1, y1] = coordinates[i]\n",
    "            local_imgs[i:i + 1] = F.interpolate(att_out[i:i + 1, :, x0:(x1+1), y0:(y1+1)], size=(448, 448),\n",
    "                                                mode='bilinear', align_corners=True)  # [N, 3, 224, 224]\n",
    "        local_fm, local_embeddings = self.pretrained_model_B(local_imgs.detach())  # [N, 2048]\n",
    "        local_logits = self.rawcls_net(local_embeddings)  # [N, 2]\n",
    "        #To find final Local image consist of object\n",
    "        \n",
    "        \n",
    "#         final_coordinates = torch.tensor(AOLM(fm.detach(), tscams.detach()))\n",
    "#         final_local_imgs = torch.zeros([batch_size, 3, 448, 448]).to(DEVICE)  # [N, 3, 448, 448]\n",
    "#         for i in range(batch_size):\n",
    "#             [x0, y0, x1, y1] = final_coordinates[i]\n",
    "#             final_local_imgs[i:i + 1] = F.interpolate(att_out[i:i + 1, :, x0:(x1+1), y0:(y1+1)], size=(448, 448),\n",
    "#                                                 mode='bilinear', align_corners=True)  # [N, 3, 224, 224]\n",
    "#         final_local_fm, final_local_embeddings, _, _ = self.pretrained_model_B(final_local_imgs.detach())  # [N, 2048]\n",
    "#         final_local_logits = self.rawcls_net(final_local_embeddings)  # [N, 2]\n",
    "        proposalN_indices, proposalN_windows_scores, window_scores \\\n",
    "            = self.APPM(self.proposalN, local_fm.detach(), ratios, window_nums_sum, N_list, iou_threshs, DEVICE)\n",
    "\n",
    "        if status == \"train\":\n",
    "            # window_imgs cls\n",
    "            window_imgs = torch.zeros([batch_size, self.proposalN, 3, 224, 224]).to(DEVICE)  # [N, 4, 3, 224, 224]\n",
    "            wnds=[]\n",
    "            for i in range(batch_size):\n",
    "                wnd=[]\n",
    "                for j in range(self.proposalN):\n",
    "                    [x0, y0, x1, y1] = coordinates_cat[proposalN_indices[i, j]]\n",
    "                    window_imgs[i:i + 1, j] = F.interpolate(local_imgs[i:i + 1, :, x0:(x1 + 1), y0:(y1 + 1)], size=(224, 224),\n",
    "                                                                mode='bilinear',\n",
    "                                                                align_corners=True)  # [N, 4, 3, 224, 224]\n",
    "                    wnd.append([x0, y0, x1, y1])\n",
    "                wnds.append(wnd)\n",
    "                   \n",
    "            window_imgs = window_imgs.reshape(batch_size * self.proposalN, 3, 224, 224)  # [N*4, 3, 224, 224]\n",
    "            _, window_embeddings = self.pretrained_model_B(window_imgs.detach())  # [N*4, 2048]\n",
    "            proposalN_windows_logits = self.rawcls_net(window_embeddings)  # [N* 4, 200]\n",
    "        else:\n",
    "            proposalN_windows_logits = torch.zeros([batch_size * self.proposalN, self.num_classes]).to(DEVICE)\n",
    "\n",
    "        return proposalN_windows_scores, proposalN_windows_logits, proposalN_indices, \\\n",
    "    window_scores, coordinates, raw_logits, local_logits, local_imgs, att_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1a302cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_imgs(org, ot, codd):\n",
    "    size=(codd[3]-codd[1], codd[2]-codd[0])\n",
    "    image_boxes2 = Image.fromarray(ot)\n",
    "    image_boxes2 = image_boxes2.resize(size)\n",
    "    imgo = Image.fromarray(org)\n",
    "    imgo.paste(image_boxes2, (codd[1], codd[0]))\n",
    "    return imgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666364d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63080361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "from torchmetrics import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bea1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6128413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import CohenKappa\n",
    "from torchmetrics import ConfusionMatrix\n",
    "from torchmetrics.functional import auc\n",
    "def classification_results(pre, tar, stat):\n",
    "    confmat = ConfusionMatrix(num_classes=2)\n",
    "    cohenkappa = CohenKappa(num_classes=2)\n",
    "    acc=Accuracy()\n",
    "    \n",
    "    print(\"*****************************\")\n",
    "    print(stat)\n",
    "    print(\"Confusion Matrix\", confmat(pre, tar).numpy())\n",
    "    print(\"cohenkappa score\", cohenkappa(pre, tar).numpy())\n",
    "    print(\"AUC\", auc(pre, tar, reorder=True).numpy())\n",
    "    print(\"accuracy\", acc(pre, tar).numpy())\n",
    "    print(\"*****************************\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a1481e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c6b4d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83895281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b3f689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs, dataloaders, dataset_sizes):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        p=[]\n",
    "        q=[]\n",
    "        for phase in ['train', 'val']:\n",
    "            \n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "#                 inputs, labels= data\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)[-1]\n",
    "                    #              output = model(input)\n",
    "                    if isinstance(outputs, tuple): # <-- inception output is a tuple (x, aux)\n",
    "                        outputs = outputs[0]  \n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "#                     print(preds)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "#                 print(preds)\n",
    "#                 print((labels.data))\n",
    "                p.append(preds)\n",
    "                q.append(labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            Q=torch.cat(q).reshape(-1).cpu()\n",
    "            P=torch.cat(p).reshape(-1).cpu()\n",
    "#             print(P)\n",
    "#             print(Q)\n",
    "            classification_results(P, Q, phase)\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "#         print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "#     model.load_state_dict(best_model_wts)\n",
    "    return best_model_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97803722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "from config import coordinates_cat, proposalN, set, vis_num\n",
    "from utils.cal_iou import calculate_iou\n",
    "from utils.vis import image_with_boxes, image_with_box\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "CFR=True\n",
    "def eval(model, testloader, criterion, status, save_path, epoch):\n",
    "    model.eval()\n",
    "    print('Evaluating')\n",
    "\n",
    "    raw_loss_sum = 0\n",
    "    local_loss_sum = 0\n",
    "#     final_local_loss_sum=0\n",
    "    windowscls_loss_sum = 0\n",
    "    total_loss_sum = 0\n",
    "    iou_corrects = 0\n",
    "    raw_correct = 0\n",
    "    local_correct = 0\n",
    "#     final_local_correct=0\n",
    "    obtain_row=[]\n",
    "#     obtain_final_local=[]\n",
    "    obtain_local=[]\n",
    "    desire=[]\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(tqdm(testloader)):\n",
    "            if set == 'CUB':\n",
    "                images, labels, boxes, scale = data\n",
    "            else:\n",
    "                images, labels = data\n",
    "            desire.append(labels)\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            proposalN_windows_score,proposalN_windows_logits, indices, \\\n",
    "            window_scores, coordinates, raw_logits, local_logits, local_imgs, \\\n",
    "            att_out = model(images, epoch, i, status)\n",
    "            raw_loss = criterion(raw_logits, labels)\n",
    "            local_loss = criterion(local_logits, labels)\n",
    "#             final_local_loss = criterion(final_local_logits,labels)\n",
    "            windowscls_loss = criterion(proposalN_windows_logits,\n",
    "                                        labels.unsqueeze(1).repeat(1, proposalN).view(-1))\n",
    "\n",
    "            total_loss = raw_loss + local_loss + windowscls_loss\n",
    "\n",
    "            raw_loss_sum += raw_loss.item()\n",
    "            local_loss_sum += local_loss.item()\n",
    "#             final_local_loss_sum += final_local_loss.item()\n",
    "            windowscls_loss_sum += windowscls_loss.item()\n",
    "\n",
    "            total_loss_sum += total_loss.item()\n",
    "            # Row branch\n",
    "\n",
    "            pred = raw_logits.max(1, keepdim=True)[1]\n",
    "            raw_correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "            obtain_row.append(pred)\n",
    "            # local branch\n",
    "            pred = local_logits.max(1, keepdim=True)[1]\n",
    "            obtain_local.append(pred)\n",
    "            local_correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "            \n",
    "#             #final_local\n",
    "#             pred = final_local_logits.max(1, keepdim=True)[1]\n",
    "#             obtain_final_local.append(pred)\n",
    "#             final_local_correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "            # object branch tensorboard\n",
    "            indices_ndarray = indices[:vis_num,:proposalN].cpu().numpy()\n",
    "            if i==0 or i==2 or i==4:\n",
    "                \n",
    "                with SummaryWriter(log_dir=os.path.join(save_path, 'log'), comment=status + 'Final Results') as writer:\n",
    "                    cat_imgs = []\n",
    "                    no_box_imgs=[]\n",
    "                    local_ims=[]\n",
    "#                     final_local_ims=[]\n",
    "                    s_box_imgs=[]\n",
    "                    for j, indice_ndarray in enumerate(indices_ndarray):\n",
    "\n",
    "                        if labels[j]==0:\n",
    "                            results=[]\n",
    "                            att=image_with_boxes(att_out[j])\n",
    "                            im = image_with_boxes(images[j])\n",
    "                            results.append(im)\n",
    "                            results.append(att)\n",
    "                            local_im = image_with_boxes(local_imgs[j])\n",
    "                            results.append(local_im)\n",
    "#                             final_local_im = image_with_boxes(final_local_imgs[j])\n",
    "#                             results.append(final_local_im)\n",
    "                            img = image_with_boxes(local_imgs[j], coordinates_cat[indice_ndarray])\n",
    "                            results.append(img)\n",
    "                            s_box_img=image_with_box(local_imgs[j], coordinates_cat[indice_ndarray])\n",
    "                            results.append(s_box_img)\n",
    "                            fin_res=combine_imgs(im, s_box_img, coordinates[j])\n",
    "                            results.append(fin_res)\n",
    "    #                         print(results)\n",
    "                            results = np.concatenate(results, axis=1)\n",
    "    #                         show_images(results, cols = 1)\n",
    "\n",
    "                            writer.add_images(status + '/' + 'Original images' +'/' + 'Local images'+ '/' +\n",
    "                                              'Object image with windows'+ str(i)+str(j), results, epoch, dataformats='HWC')\n",
    "\n",
    "    raw_loss_avg = raw_loss_sum / (i+1)\n",
    "    local_loss_avg = local_loss_sum / (i+1)\n",
    "#     final_local_loss_avg = final_local_loss_sum / (i+1)\n",
    "    windowscls_loss_avg = windowscls_loss_sum / (i+1)\n",
    "    total_loss_avg = total_loss_sum / (i+1)\n",
    "\n",
    "    raw_accuracy = raw_correct / len(testloader.dataset)\n",
    "    local_accuracy = local_correct / len(testloader.dataset)\n",
    "#     final_local_accuracy = final_local_correct / len(testloader.dataset)\n",
    "\n",
    "    \n",
    "    if CFR==True:\n",
    "        tar=torch.cat(desire).reshape(-1).cpu()\n",
    "        o_r=torch.cat(obtain_row).reshape(-1).cpu()\n",
    "#         o_fl=torch.cat(obtain_final_local).reshape(-1).cpu()\n",
    "        o_l=torch.cat(obtain_local).reshape(-1).cpu()\n",
    "        classification_results(o_r, tar, \"Row\")\n",
    "#         classification_results(o_fl, tar, \"Final_Local\")\n",
    "        classification_results(o_l, tar, \"Local\")\n",
    "    return raw_loss_avg, windowscls_loss_avg, total_loss_avg, raw_accuracy, local_accuracy, local_loss_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caf6c28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MainNet(proposalN=proposalN, num_classes=num_classes, channels=2048, theta=(0.6), red_p=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228677a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc41e968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f565e523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7e12fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74e3d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "from config import max_checkpoint_num, proposalN, eval_trainset, set\n",
    "# from utils.eval_model import eval\n",
    "\n",
    "def train(model,\n",
    "          trainloader,\n",
    "          testloader,\n",
    "          criterion,\n",
    "          optimizer,\n",
    "          scheduler,\n",
    "          save_path,\n",
    "          start_epoch,\n",
    "          end_epoch,\n",
    "          num_epochs,\n",
    "          save_interval):\n",
    "#     dataset_sizes = {['train', 'val'][x]: [len(trainloader.dataset),len(testloader.dataset)][x] for x in range(len(['train', 'val']))}\n",
    "    \n",
    "#     dataloaders = {['train', 'val'][x]: [trainloader,testloader][x] for x in range(len(['train', 'val']))}\n",
    "    \n",
    "#     print(\"Exploring...\")\n",
    "#     if start_epoch < 2:\n",
    "#         best_model_wts = train_model(model.pretrained_model_B, criterion, optimizer, scheduler, num_epochs,\n",
    "#                                            dataloaders, dataset_sizes)\n",
    "#         model.pretrained_model_B.load_state_dict(best_model_wts)\n",
    "    print(\"Expolaration done.\")\n",
    "    for epoch in range(start_epoch + 1, end_epoch + 1):\n",
    "        model.train()\n",
    "        \n",
    "        print('Training %d epoch' % epoch)\n",
    "        \n",
    "        lr = next(iter(optimizer.param_groups))['lr']\n",
    "\n",
    "        for i, data in enumerate(tqdm(trainloader)):\n",
    "            if set == 'CUB':\n",
    "                images, labels, _, _ = data\n",
    "            else:\n",
    "                images, labels = data\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            proposalN_windows_score, proposalN_windows_logits, indices, \\\n",
    "            window_scores, _, raw_logits, local_logits, local_imgs, _ = model(images, epoch, i, 'train')\n",
    "\n",
    "            raw_loss = criterion(raw_logits, labels)\n",
    "            local_loss = criterion(local_logits, labels)\n",
    "#             final_local_loss=criterion(final_local_logits, labels)\n",
    "            windowscls_loss = criterion(proposalN_windows_logits,\n",
    "                               labels.unsqueeze(1).repeat(1, proposalN).view(-1))\n",
    "\n",
    "            if epoch < 30:\n",
    "                total_loss = raw_loss\n",
    "            else:\n",
    "                total_loss = raw_loss + local_loss + windowscls_loss\n",
    "\n",
    "            total_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # evaluation every epoch\n",
    "        if eval_trainset:\n",
    "            raw_loss_avg, windowscls_loss_avg, total_loss_avg, raw_accuracy, local_accuracy, local_loss_avg = eval(model, trainloader, criterion, 'train', save_path, epoch)\n",
    "\n",
    "            print(\n",
    "                'Train set: raw accuracy: {:.2f}%, local accuracy: {:.2f}%'.format(\n",
    "                    100. * raw_accuracy, 100. * local_accuracy))\n",
    "\n",
    "            # tensorboard\n",
    "            with SummaryWriter(log_dir=os.path.join(save_path, 'log'), comment='train') as writer:\n",
    "\n",
    "                writer.add_scalar('Train/learning rate', lr, epoch)\n",
    "                writer.add_scalar('Train/raw_accuracy', raw_accuracy, epoch)\n",
    "                writer.add_scalar('Train/local_accuracy', local_accuracy, epoch)\n",
    "#                 writer.add_scalar('Train/final_local_accuracy', final_local_accuracy, epoch)\n",
    "                writer.add_scalar('Train/raw_loss_avg', raw_loss_avg, epoch)\n",
    "                writer.add_scalar('Train/local_loss_avg', local_loss_avg, epoch)\n",
    "#                 writer.add_scalar('Train/final_local_loss_avg', final_local_loss_avg, epoch)\n",
    "                writer.add_scalar('Train/windowscls_loss_avg', windowscls_loss_avg, epoch)\n",
    "                writer.add_scalar('Train/total_loss_avg', total_loss_avg, epoch)\n",
    "                \n",
    "\n",
    "        # eval testset\n",
    "        raw_loss_avg, windowscls_loss_avg, total_loss_avg, raw_accuracy, local_accuracy, local_loss_avg = eval(model, testloader, criterion, 'test', save_path, epoch)\n",
    "\n",
    "        print(\n",
    "            'Test set: raw accuracy: {:.2f}%, local accuracy: {:.2f}%'.format(\n",
    "                100. * raw_accuracy, 100. * local_accuracy))\n",
    "\n",
    "        # tensorboard\n",
    "        with SummaryWriter(log_dir=os.path.join(save_path, 'log'), comment='test') as writer:\n",
    "            writer.add_scalar('Test/raw_accuracy', raw_accuracy, epoch)\n",
    "            writer.add_scalar('Test/local_accuracy', local_accuracy, epoch)\n",
    "#             writer.add_scalar('Test/Final_local_accuracy', final_local_accuracy, epoch)\n",
    "            writer.add_scalar('Test/raw_loss_avg', raw_loss_avg, epoch)\n",
    "            writer.add_scalar('Test/local_loss_avg', local_loss_avg, epoch)\n",
    "#             writer.add_scalar('Test/Final_local_accuracy', final_local_loss_avg, epoch)\n",
    "            writer.add_scalar('Test/windowscls_loss_avg', windowscls_loss_avg, epoch)\n",
    "            writer.add_scalar('Test/total_loss_avg', total_loss_avg, epoch)\n",
    "\n",
    "        # save checkpoint\n",
    "        if (epoch % save_interval == 0) or (epoch == end_epoch):\n",
    "            print('Saving checkpoint')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'learning_rate': lr,\n",
    "            }, os.path.join(save_path, 'epoch' + str(epoch) + '.pth'))\n",
    "\n",
    "        # Limit the number of checkpoints to less than or equal to max_checkpoint_num,\n",
    "        # and delete the redundant ones\n",
    "        checkpoint_list = [os.path.basename(path) for path in glob.glob(os.path.join(save_path, '*.pth'))]\n",
    "        if len(checkpoint_list) == max_checkpoint_num + 1:\n",
    "            idx_list = [int(name.replace('epoch', '').replace('.pth', '')) for name in checkpoint_list]\n",
    "            min_idx = min(idx_list)\n",
    "            os.remove(os.path.join(save_path, 'epoch' + str(min_idx) + '.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5083452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image as im\n",
    "def tenssor2array(images):\n",
    "    if type(images) is not np.ndarray:\n",
    "        image = images.clone().detach()\n",
    "\n",
    "        rgbN = [(255, 0, 0), (255, 165, 0), (255, 255, 0), (0, 255, 0), (0, 255, 0), (0, 255, 0), (0, 255, 0)]\n",
    "\n",
    "        # Anti-normalization\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        image[0] = image[0] * std[0] + mean[0]\n",
    "        image[1] = image[1] * std[1] + mean[1]\n",
    "        image[2] = image[2].mul(std[2]) + mean[2]\n",
    "        image = image.mul(255).byte()\n",
    "\n",
    "        image = image.data.cpu().numpy()\n",
    "\n",
    "        image.astype(np.uint8)\n",
    "\n",
    "        image = np.transpose(image, (1, 2, 0))  # CHW --> HWC\n",
    "        \n",
    "    return im.fromarray(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a019599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.8\n",
    "GAMMA = 2\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, alpha=ALPHA, gamma=GAMMA, smooth=1):\n",
    "\n",
    "        \n",
    "        #first compute binary cross-entropy \n",
    "        BCE = nn.CrossEntropyLoss()(inputs, targets)\n",
    "        BCE_EXP = torch.exp(-BCE)\n",
    "        focal_loss = alpha * (1-BCE_EXP)**gamma * BCE\n",
    "                       \n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c11612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "163695f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53f0167d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "346e34f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cce4b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "567d4bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TUMOR():\n",
    "    def __init__(self, input_size, root, is_train=True, data_len=None):\n",
    "        self.input_size = input_size\n",
    "        self.root = root\n",
    "#         self.transform = transform\n",
    "#         self.to_pil = transforms.ToPILImage()\n",
    "        self.is_train = is_train\n",
    "        train_img_path = os.path.join(self.root)\n",
    "        test_img_path = os.path.join(self.root)\n",
    "        train_label_file = open(os.path.join(self.root, 'train.txt'))\n",
    "        test_label_file = open(os.path.join(self.root, 'test.txt'))\n",
    "        train_img_label = []\n",
    "        test_img_label = []\n",
    "        for line in train_label_file:\n",
    "            train_img_label.append([os.path.join(train_img_path, line[:-2]), int(line[-2])])\n",
    "        for line in test_label_file:\n",
    "            test_img_label.append([os.path.join(test_img_path, line[:-2]), int(line[-2])])\n",
    "        self.train_img_label = train_img_label[:data_len]\n",
    "        self.test_img_label = test_img_label[:data_len]\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.is_train:\n",
    "            img, target = imageio.imread(self.train_img_label[index][0]), self.train_img_label[index][1]\n",
    "            if len(img.shape) == 2:\n",
    "                img = np.stack([img] * 3, 2)\n",
    "            img = Image.fromarray(img, mode='RGB')\n",
    "\n",
    "            img = transforms.Resize((self.input_size, self.input_size), Image.Resampling.BILINEAR)(img)\n",
    "            # img = transforms.RandomResizedCrop(size=self.input_size,scale=(0.4, 0.75),ratio=(0.5,1.5))(img)\n",
    "            # img = transforms.RandomCrop(self.input_size)(img)\n",
    "            img = transforms.RandomHorizontalFlip()(img)\n",
    "            img = transforms.ColorJitter(brightness=0.2, contrast=0.2)(img)\n",
    "\n",
    "            img = transforms.ToTensor()(img)\n",
    "            img = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(img)\n",
    "\n",
    "        else:\n",
    "            img, target = imageio.imread(self.test_img_label[index][0]), self.test_img_label[index][1]\n",
    "            if len(img.shape) == 2:\n",
    "                img = np.stack([img] * 3, 2)\n",
    "            img = Image.fromarray(img, mode='RGB')\n",
    "            img = transforms.Resize((self.input_size, self.input_size), Image.BILINEAR)(img)\n",
    "            # img = transforms.CenterCrop(self.input_size)(img)\n",
    "            img = transforms.ToTensor()(img)\n",
    "            img = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(img)\n",
    "\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.is_train:\n",
    "            return len(self.train_img_label)\n",
    "        else:\n",
    "            return len(self.test_img_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c41e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=TUMOR(input_size, root, is_train=True, data_len=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19d1b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train_label_file:\n",
    "#     print(os.path.join(train_img_path, line[:-2]), int(line[-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8929f86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=torch.rand(1, 3, 448, 448)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e00a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d=model.pretrained_model_B(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc176589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TUMOR'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d54b4a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MURA trainset\n",
      "Loading MURA testset\n"
     ]
    }
   ],
   "source": [
    "trainloader, testloader = read_dataset(input_size, batch_size, root, set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45c09f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1832"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fbf649ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MainNet(proposalN=proposalN, num_classes=num_classes, channels=2048, theta=(0.6), red_p=0.15)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64d75b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MURA trainset\n",
      "Loading MURA testset\n"
     ]
    }
   ],
   "source": [
    "trainloader, testloader = read_dataset(input_size, batch_size, root, set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "832160d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_x, _ = next(iter(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b10f8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([10, 3, 448, 448])\n",
      "Labels batch shape: torch.Size([10])\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(trainloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "# plt.imshow(img, cmap=\"gray\")\n",
    "# plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04d3432b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MURA trainset\n",
      "Loading MURA testset\n",
      "Load model from ./checkpoint/TUMOR\\six\\epoch200.pth\n",
      "Resume from ./checkpoint/TUMOR\\six\\epoch200.pth\n",
      "Expolaration done.\n",
      "Training 201 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 367/367 [02:33<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 367/367 [01:01<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************\n",
      "Row\n",
      "Confusion Matrix [[899   9]\n",
      " [  0 924]]\n",
      "cohenkappa score 0.9901731\n",
      "AUC 0.5\n",
      "accuracy 0.9950873\n",
      "*****************************\n",
      "*****************************\n",
      "Local\n",
      "Confusion Matrix [[880  28]\n",
      " [  3 921]]\n",
      "cohenkappa score 0.9661465\n",
      "AUC 0.5\n",
      "accuracy 0.9830786\n",
      "*****************************\n",
      "Train set: raw accuracy: 99.51%, local accuracy: 98.31%\n",
      "Evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 240/240 [00:32<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************\n",
      "Row\n",
      "Confusion Matrix [[714 112]\n",
      " [  0 371]]\n",
      "cohenkappa score 0.79805136\n",
      "AUC 0.5\n",
      "accuracy 0.90643275\n",
      "*****************************\n",
      "*****************************\n",
      "Local\n",
      "Confusion Matrix [[702 124]\n",
      " [  2 369]]\n",
      "cohenkappa score 0.77435434\n",
      "AUC 0.5\n",
      "accuracy 0.8947368\n",
      "*****************************\n",
      "Test set: raw accuracy: 90.64%, local accuracy: 89.47%\n",
      "Saving checkpoint\n",
      "Training 202 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|                                                               | 81/367 [00:44<02:36,  1.83it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 69>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m     train(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     58\u001b[0m           trainloader\u001b[38;5;241m=\u001b[39mtrainloader,\n\u001b[0;32m     59\u001b[0m           testloader\u001b[38;5;241m=\u001b[39mtestloader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m           num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[0;32m     67\u001b[0m           save_interval\u001b[38;5;241m=\u001b[39msave_interval)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 70\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [39]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m time_str \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     55\u001b[0m shutil\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./config.py\u001b[39m\u001b[38;5;124m'\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124mconfig.py\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(time_str)))\n\u001b[1;32m---> 57\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m      \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m      \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m      \u001b[49m\u001b[43mend_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m      \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_interval\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, trainloader, testloader, criterion, optimizer, scheduler, save_path, start_epoch, end_epoch, num_epochs, save_interval)\u001b[0m\n\u001b[0;32m     58\u001b[0m         total_loss \u001b[38;5;241m=\u001b[39m raw_loss \u001b[38;5;241m+\u001b[39m local_loss \u001b[38;5;241m+\u001b[39m windowscls_loss\n\u001b[0;32m     60\u001b[0m     total_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 62\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# evaluation every epoch\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en_2\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:68\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en_2\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en_2\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en_2\\lib\\site-packages\\torch\\optim\\sgd.py:151\u001b[0m, in \u001b[0;36mSGD.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m             momentum_buffer_list\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmomentum_buffer\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 151\u001b[0m \u001b[43msgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmomentum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdampening\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnesterov\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, momentum_buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en_2\\lib\\site-packages\\torch\\optim\\sgd.py:202\u001b[0m, in \u001b[0;36msgd\u001b[1;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_sgd\n\u001b[1;32m--> 202\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m     \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdampening\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m     \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnesterov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en_2\\lib\\site-packages\\torch\\optim\\sgd.py:229\u001b[0m, in \u001b[0;36m_single_tensor_sgd\u001b[1;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[0;32m    226\u001b[0m d_p \u001b[38;5;241m=\u001b[39m d_p_list[i] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m maximize \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39md_p_list[i]\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 229\u001b[0m     d_p \u001b[38;5;241m=\u001b[39m \u001b[43md_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m momentum \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    232\u001b[0m     buf \u001b[38;5;241m=\u001b[39m momentum_buffer_list[i]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#coding=utf-8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import shutil\n",
    "import time\n",
    "from config import num_classes, model_name, model_path, lr_milestones, lr_decay_rate, input_size, \\\n",
    "    root, end_epoch, save_interval, init_lr, batch_size, CUDA_VISIBLE_DEVICES, weight_decay, \\\n",
    "    proposalN, set, channels\n",
    "# from utils.train_model import train\n",
    "from utils.read_dataset import read_dataset\n",
    "from utils.auto_laod_resume import auto_load_resume\n",
    "# from networks.model import MainNet\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def main():\n",
    "    batch_size = 5\n",
    "    end_epoch = 202\n",
    "    input_size = 448\n",
    "    set='TUMOR'\n",
    "    model_name='six'\n",
    "    num_epochs=30\n",
    "    trainloader, testloader = read_dataset(input_size, batch_size, root, set, subset='HAND')\n",
    "#     batch = next(iter(trainloader))\n",
    "    \n",
    "    model = MainNet(proposalN=proposalN, num_classes=num_classes, channels=2048, theta=(0.6), red_p=0.15)\n",
    "\n",
    "\n",
    "    criterion = FocalLoss()\n",
    "\n",
    "    parameters = model.parameters()\n",
    "\n",
    "   \n",
    "    save_path = os.path.join(model_path, model_name)\n",
    "    if os.path.exists(save_path):\n",
    "        start_epoch, lr = auto_load_resume(model, save_path, status='train')\n",
    "        assert start_epoch < end_epoch\n",
    "    else:\n",
    "        os.makedirs(save_path)\n",
    "        start_epoch = 0\n",
    "        lr = init_lr\n",
    "\n",
    "    # define optimizers\n",
    "    optimizer = torch.optim.SGD(parameters, lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "\n",
    "    model = model.cuda()\n",
    "\n",
    "    scheduler = MultiStepLR(optimizer, milestones=lr_milestones, gamma=lr_decay_rate)\n",
    "#     make_dot(yhat, params=dict(list(model.named_parameters()))).render(\"MainModel_torchviz\", format=\"png\")\n",
    "\n",
    "    time_str = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    shutil.copy('./config.py', os.path.join(save_path, \"{}config.py\".format(time_str)))\n",
    "\n",
    "    train(model=model,\n",
    "          trainloader=trainloader,\n",
    "          testloader=testloader,\n",
    "          criterion=criterion,\n",
    "          optimizer=optimizer,\n",
    "          scheduler=scheduler,\n",
    "          save_path=save_path,\n",
    "          start_epoch=start_epoch,\n",
    "          end_epoch=end_epoch,\n",
    "          num_epochs=30,\n",
    "          save_interval=save_interval)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a0dd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='six'\n",
    "save_path = os.path.join(model_path, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02bfbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import shutil\n",
    "import time\n",
    "from config import num_classes, model_name, model_path, lr_milestones, lr_decay_rate, input_size, \\\n",
    "    root, end_epoch, save_interval, init_lr, batch_size, CUDA_VISIBLE_DEVICES, weight_decay, \\\n",
    "    proposalN, set, channels\n",
    "# from utils.train_model import train\n",
    "from utils.read_dataset import read_dataset\n",
    "from utils.auto_laod_resume import auto_load_resume\n",
    "# from networks.model import MainNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1836b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "def test_model(model, dataloaders, device):\n",
    "    CM=0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in dataloaders['test']:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images) #file_name\n",
    "            preds = torch.argmax(outputs.data, 1)\n",
    "            CM+=confusion_matrix(labels.cpu(), preds.cpu(),labels=[0,1])\n",
    "            \n",
    "        tn=CM[0][0]\n",
    "        tp=CM[1][1]\n",
    "        fp=CM[0][1]\n",
    "        fn=CM[1][0]\n",
    "        acc=np.sum(np.diag(CM)/np.sum(CM))\n",
    "        sensitivity=tp/(tp+fn)\n",
    "        precision=tp/(tp+fp)\n",
    "        \n",
    "        print('\\nTestset Accuracy(mean): %f %%' % (100 * acc))\n",
    "        print()\n",
    "        print('Confusion Matirx : ')\n",
    "        print(CM)\n",
    "        print('- Sensitivity : ',(tp/(tp+fn))*100)\n",
    "        print('- Specificity : ',(tn/(tn+fp))*100)\n",
    "        print('- Precision: ',(tp/(tp+fp))*100)\n",
    "        print('- NPV: ',(tn/(tn+fn))*100)\n",
    "        print('- F1 : ',((2*sensitivity*precision)/(sensitivity+precision))*100)\n",
    "        print()\n",
    "                \n",
    "    return acc, CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e0852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MainNet(proposalN=proposalN, num_classes=num_classes, channels=2048, theta=(0.6), red_p=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63db0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='one'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62870fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, testloader = read_dataset(input_size, batch_size, root, set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529bd801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc0794",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(model_path, model_name)\n",
    "if os.path.exists(save_path):\n",
    "    start_epoch, lr = auto_load_resume(model, save_path, status='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faafde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = {['train', 'test'][x]: [len(trainloader.dataset),len(testloader.dataset)][x] for x in range(len(['train', 'val']))}\n",
    "dataloaders = {['train', 'test'][x]: [trainloader,testloader][x] for x in range(len(['train', 'val']))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac39b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2b715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1af1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from config import input_size, root, proposalN, channels\n",
    "from utils.read_dataset import read_dataset\n",
    "from utils.auto_laod_resume import auto_load_resume\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if CUDA else \"cpu\")\n",
    "\n",
    "#load dataset\n",
    "_, testloader = read_dataset(input_size, batch_size, root, set)\n",
    "\n",
    "\n",
    "model = MainNet(proposalN=proposalN, num_classes=num_classes, channels=2048, theta=(0.6), red_p=0.15)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "if os.path.exists(save_path):\n",
    "    epoch = auto_load_resume(model, save_path, status='train')\n",
    "else:\n",
    "    sys.exit('There is not a pth exist.')\n",
    "\n",
    "print('Testing')\n",
    "raw_correct = 0\n",
    "object_correct = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(tqdm(testloader)):\n",
    "        if set == 'CUB':\n",
    "            x, y, boxes, _ = data\n",
    "        else:\n",
    "            x, y = data\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        local_logits, local_imgs = model(x, epoch, i, 'train', DEVICE)[-3:-1]\n",
    "        # local\n",
    "        pred = local_logits.max(1, keepdim=True)[1]\n",
    "        object_correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    print('\\nObject branch accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            object_correct, len(testloader.dataset), 100. * object_correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a22bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd04479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
