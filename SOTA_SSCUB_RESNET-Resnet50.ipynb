{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "577fc625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967e5156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fe4d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n",
    "import random\n",
    "from utils.read_dataset import read_dataset\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5850e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import num_classes, model_name, model_path, lr_milestones, lr_decay_rate, input_size, \\\n",
    "    root, end_epoch, save_interval, init_lr, batch_size, CUDA_VISIBLE_DEVICES, weight_decay, \\\n",
    "    proposalN, set, channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cb24d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d659caa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9acb944f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0.dev20221118+cu116\n"
     ]
    }
   ],
   "source": [
    "print(torch. __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397fd5ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25653b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef4611bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pop=r\"C:\\Users\\Admin\\Dropbox\\PC\\Documents\\FGVC_MSFM\\MMAL-Net\\datasets\\multiclass_path\\train_positive.txt\"\n",
    "# nop=r\"C:\\Users\\Admin\\Dropbox\\PC\\Documents\\FGVC_MSFM\\MMAL-Net\\datasets\\multiclass_path\\train_negative.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa218e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf=pd.read_table(nop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "489c0ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf=pdf.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c47c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torch.nn import init\n",
    "import torch\n",
    "\n",
    "__all__ = ['xception']\n",
    "\n",
    "model_urls = {\n",
    "    'xception':'https://www.dropbox.com/s/1hplpzet9d7dv29/xception-c0a72b38.pth.tar?dl=1'\n",
    "}\n",
    "\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,dilation=1,bias=False):\n",
    "        super(SeparableConv2d,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n",
    "        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,in_filters,out_filters,reps,strides=1,start_with_relu=True,grow_first=True):\n",
    "        super(Block, self).__init__()\n",
    "\n",
    "        if out_filters != in_filters or strides!=1:\n",
    "            self.skip = nn.Conv2d(in_filters,out_filters,1,stride=strides, bias=False)\n",
    "            self.skipbn = nn.BatchNorm2d(out_filters)\n",
    "        else:\n",
    "            self.skip=None\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        rep=[]\n",
    "\n",
    "        filters=in_filters\n",
    "        if grow_first:\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n",
    "            rep.append(nn.BatchNorm2d(out_filters))\n",
    "            filters = out_filters\n",
    "\n",
    "        for i in range(reps-1):\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv2d(filters,filters,3,stride=1,padding=1,bias=False))\n",
    "            rep.append(nn.BatchNorm2d(filters))\n",
    "        \n",
    "        if not grow_first:\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n",
    "            rep.append(nn.BatchNorm2d(out_filters))\n",
    "\n",
    "        if not start_with_relu:\n",
    "            rep = rep[1:]\n",
    "        else:\n",
    "            rep[0] = nn.ReLU(inplace=False)\n",
    "\n",
    "        if strides != 1:\n",
    "            rep.append(nn.MaxPool2d(3,strides,1))\n",
    "        self.rep = nn.Sequential(*rep)\n",
    "\n",
    "    def forward(self,inp):\n",
    "        x = self.rep(inp)\n",
    "\n",
    "        if self.skip is not None:\n",
    "            skip = self.skip(inp)\n",
    "            skip = self.skipbn(skip)\n",
    "        else:\n",
    "            skip = inp\n",
    "\n",
    "        x+=skip\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class Xception(nn.Module):\n",
    "    \"\"\"\n",
    "    Xception optimized for the ImageNet dataset, as specified in\n",
    "    https://arxiv.org/pdf/1610.02357.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=1000):\n",
    "        \"\"\" Constructor\n",
    "        Args:\n",
    "            num_classes: number of classes\n",
    "        \"\"\"\n",
    "        super(Xception, self).__init__()\n",
    "\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3,2, 0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32,64,3,bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        #do relu here\n",
    "\n",
    "        self.block1=Block(64,128,2,2,start_with_relu=False,grow_first=True)\n",
    "        self.block2=Block(128,256,2,2,start_with_relu=True,grow_first=True)\n",
    "        self.block3=Block(256,728,2,2,start_with_relu=True,grow_first=True)\n",
    "\n",
    "        self.block4=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block5=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block6=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block7=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "\n",
    "        self.block8=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block9=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block10=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block11=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "\n",
    "        self.block12=Block(728,1024,2,2,start_with_relu=True,grow_first=False)\n",
    "\n",
    "        self.conv3 = SeparableConv2d(1024,1536,3,1,1)\n",
    "        self.bn3 = nn.BatchNorm2d(1536)\n",
    "\n",
    "        #do relu here\n",
    "        self.conv4 = SeparableConv2d(1536,2048,3,1,1)\n",
    "        self.bn4 = nn.BatchNorm2d(2048)\n",
    "\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "        #------- init weights --------\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "        #-----------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        x = self.block7(x)\n",
    "        x = self.block8(x)\n",
    "        x = self.block9(x)\n",
    "        x = self.block10(x)\n",
    "        x = self.block11(x)\n",
    "        x = self.block12(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        fm = x\n",
    "#         x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.fc(x)\n",
    "\n",
    "        return fm\n",
    "\n",
    "\n",
    "\n",
    "def xception(pretrained=False, pth_path='', **kwargs):\n",
    "    \"\"\"\n",
    "    Construct Xception.\n",
    "    \"\"\"\n",
    "\n",
    "    model = Xception(**kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = torch.load(pth_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "#         model.load_state_dict(model_zoo.load_url(model_urls['xception']))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5228fceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xception(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0fe9b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.rand(1, 3, 448, 448)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54cb69b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "416733f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 14, 14])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c3acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d317368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1f012ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n",
    "           'wide_resnet50_2', 'wide_resnet101_2']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
    "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
    "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
    "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, return_cam=True, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "#         self.SSPCAB_Block = SSPCAB(2048)\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.num_classes=num_classes\n",
    "        self.base_width = width_per_group\n",
    "        self.return_cam=return_cam\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        # self.conv2 = nn.Conv2d(in_channels=2048, out_channels=2048, kernel_size=4, stride=1,bias=False)\n",
    "        # self.conv3 = nn.Conv2d(in_channels=2048, out_channels=201, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "#         self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(512 * block.expansion, self.num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x=ViT('B_16')(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "#         conv5_b = self.layer4[:2](x)\n",
    "#         x = self.layer4[2](conv5_b)\n",
    "\n",
    "        fm1 = x\n",
    "#         fm2 = self.SSPCAB_Block(fm1)\n",
    "#         x = self.avgpool(fm2)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.dropout(x)\n",
    "       \n",
    "#         embeeding = x\n",
    "#         logs=self.fc(embeeding)\n",
    "        \n",
    "        return fm1\n",
    "\n",
    "\n",
    "def _resnet(arch, block, layers, pretrained, pth_path, **kwargs):\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = torch.load(pth_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet18(pth_path, pretrained=False, **kwargs):\n",
    "    r\"\"\"ResNet-18 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, pth_path,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def resnet34(pth_path, pretrained=False, **kwargs):\n",
    "    r\"\"\"ResNet-34 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, pth_path,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def resnet50(pth_path, pretrained=False, **kwargs):\n",
    "    r\"\"\"ResNet-50 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3],pretrained, pth_path,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def resnet101(pth_path, pretrained=False, **kwargs):\n",
    "    r\"\"\"ResNet-101 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], pretrained, pth_path,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def resnet152(pth_path, pretrained=False, **kwargs):\n",
    "    r\"\"\"ResNet-152 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, pth_path,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def resnext50_32x4d(pth_path, pretrained=False, **kwargs):\n",
    "    r\"\"\"ResNeXt-50 32x4d model from\n",
    "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 4\n",
    "    return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3],\n",
    "                   pretrained, pth_path, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def resnext101_32x8d(pth_path, pretrained=False, **kwargs):\n",
    "    r\"\"\"ResNeXt-101 32x8d model from\n",
    "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 8\n",
    "    return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3],\n",
    "                   pretrained, pth_path, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def wide_resnet50_2(pth_path, pretrained=False, **kwargs):\n",
    "    r\"\"\"Wide ResNet-50-2 model from\n",
    "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n",
    "\n",
    "    The model is the same as ResNet except for the bottleneck number of channels\n",
    "    which is twice larger in every block. The number of channels in outer 1x1\n",
    "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
    "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['width_per_group'] = 64 * 2\n",
    "    return _resnet('wide_resnet50_2', Bottleneck, [3, 4, 6, 3],\n",
    "                   pretrained, pth_path, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def wide_resnet101_2(pth_path, pretrained=False, **kwargs):\n",
    "    r\"\"\"Wide ResNet-101-2 model from\n",
    "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n",
    "\n",
    "    The model is the same as ResNet except for the bottleneck number of channels\n",
    "    which is twice larger in every block. The number of channels in outer 1x1\n",
    "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
    "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['width_per_group'] = 64 * 2\n",
    "    return _resnet('wide_resnet101_2', Bottleneck, [3, 4, 23, 3],\n",
    "                   pretrained, pth_path, **kwargs)\n",
    "# Squeeze and Excitation block\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, num_channels, reduction_ratio=8):\n",
    "        '''\n",
    "            num_channels: The number of input channels\n",
    "            reduction_ratio: The reduction ratio 'r' from the paper\n",
    "        '''\n",
    "        super(SELayer, self).__init__()\n",
    "        num_channels_reduced = num_channels // reduction_ratio\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "        self.fc1 = nn.Linear(num_channels, num_channels_reduced, bias=True)\n",
    "        self.fc2 = nn.Linear(num_channels_reduced, num_channels, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        batch_size, num_channels, H, W = input_tensor.size()\n",
    "\n",
    "        squeeze_tensor = input_tensor.view(batch_size, num_channels, -1).mean(dim=2)\n",
    "#         print(squeeze_tensor.shape)\n",
    "        # channel excitation\n",
    "        fc_out_1 = self.relu(self.fc1(squeeze_tensor))\n",
    "#         print(fc_out_1.shape)\n",
    "        fc_out_2 = self.sigmoid(self.fc2(fc_out_1))\n",
    "#         print(fc_out_2.shape)\n",
    "        a, b = squeeze_tensor.size()\n",
    "#         print(a,b)\n",
    "#         fc_out_2 = torch.reshape(fc_out_2, a, b, 1, 1)\n",
    "        output_tensor = torch.mul(input_tensor, fc_out_2.view(a, b, 1, 1))\n",
    "        return output_tensor\n",
    "\n",
    "\n",
    "# SSPCAB implementation\n",
    "class SSPCAB(nn.Module):\n",
    "    def __init__(self, channels, kernel_dim=1, dilation=1, reduction_ratio=4):\n",
    "        '''\n",
    "            channels: The number of filter at the output (usually the same with the number of filter from the input)\n",
    "            kernel_dim: The dimension of the sub-kernels ' k' ' from the paper\n",
    "            dilation: The dilation dimension 'd' from the paper\n",
    "            reduction_ratio: The reduction ratio for the SE block ('r' from the paper)\n",
    "        '''\n",
    "        super(SSPCAB, self).__init__()\n",
    "        self.pad = kernel_dim + dilation\n",
    "        self.border_input = kernel_dim + 2*dilation + 1\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.se = SELayer(channels, reduction_ratio=reduction_ratio)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels,\n",
    "                               out_channels=channels,\n",
    "                               kernel_size=kernel_dim)\n",
    "        self.conv2 = nn.Conv2d(in_channels=channels,\n",
    "                               out_channels=channels,\n",
    "                               kernel_size=kernel_dim)\n",
    "        self.conv3 = nn.Conv2d(in_channels=channels,\n",
    "                               out_channels=channels,\n",
    "                               kernel_size=kernel_dim)\n",
    "        self.conv4 = nn.Conv2d(in_channels=channels,\n",
    "                               out_channels=channels,\n",
    "                               kernel_size=kernel_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = F.pad(x, (self.pad, self.pad, self.pad, self.pad), \"constant\", 0)\n",
    "\n",
    "#         x1 = self.conv1(x[:, :, :-self.border_input, :-self.border_input])\n",
    "#         x2 = self.conv2(x[:, :, self.border_input:, :-self.border_input])\n",
    "#         x3 = self.conv3(x[:, :, :-self.border_input, self.border_input:])\n",
    "#         x4 = self.conv4(x[:, :, self.border_input:, self.border_input:])\n",
    "#         x = self.relu(x1 + x2 + x3 + x4)\n",
    "\n",
    "        x = self.se(x)\n",
    "        return x\n",
    "\n",
    "class MyEnsemble(nn.Module):\n",
    "    def __init__(self, modelA, modelB):\n",
    "        super(MyEnsemble, self).__init__()\n",
    "        self.modelA = modelA\n",
    "        self.modelB = modelB\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.modelA(x)\n",
    "        x = self.modelB(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15364611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from skimage import measure\n",
    "x_test=torch.rand(1, 3, 448, 448)\n",
    "pretrain_path_B='./models/pretrained/resnet50-19c8e357.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "560ea594",
   "metadata": {},
   "outputs": [],
   "source": [
    "fms=torch.rand(1, 2, 14, 14)\n",
    "fm1=torch.rand(1, 2048, 14, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01fdb9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod=resnet50(pretrain_path_B, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1d87168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test=mod(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56e5de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test_1=mod(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5bce889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.sigmoid(y_test_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "800d42af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4db30fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y=A[0][0][:, 0], A[0][0][:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfe42d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ce158f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1a4ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e4315c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from skimage import measure\n",
    "def CFM2(fms):\n",
    "    A = torch.sum(fms, dim=1, keepdim=True)\n",
    "    a = torch.mean(A, dim=[2, 3], keepdim=True)\n",
    "    M = (A > a).float()\n",
    "\n",
    "    coordinates = []\n",
    "    for i, m in enumerate(M):\n",
    "        mask_np = m.cpu().numpy().reshape(14, 14)\n",
    "        component_labels = measure.label(mask_np)\n",
    "\n",
    "        properties = measure.regionprops(component_labels)\n",
    "        areas = []\n",
    "        for prop in properties:\n",
    "            areas.append(prop.area)\n",
    "        \n",
    "        max_idx = areas.index(max(areas))\n",
    "\n",
    "\n",
    "        intersection = ((component_labels==(max_idx+1)).astype(int))==1\n",
    "        \n",
    "        prop = measure.regionprops(intersection.astype(int))\n",
    "          \n",
    "        if len(prop) == 0:\n",
    "            bbox = [0, 0, 14, 14]\n",
    "            print('there is one img no intersection')\n",
    "        else:\n",
    "            \n",
    "            bbox = prop[0].bbox\n",
    "            print(bbox)\n",
    "#         print(\"B box is\",bbox)\n",
    "        x_lefttop = bbox[0] * 32 - 1\n",
    "        y_lefttop = bbox[1] * 32 - 1\n",
    "        x_rightlow = bbox[2] * 32 - 1\n",
    "        y_rightlow = bbox[3] * 32 - 1\n",
    "        # for image\n",
    "        if x_lefttop < 0:\n",
    "            x_lefttop = 0\n",
    "        if y_lefttop < 0:\n",
    "            y_lefttop = 0\n",
    "        coordinate = [x_lefttop, y_lefttop, x_rightlow, y_rightlow]\n",
    "        coordinates.append(coordinate)\n",
    "    return coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3906f85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from skimage import measure\n",
    "def AOLM(fms, fm1):\n",
    "    A = torch.sum(fms, dim=1, keepdim=True)\n",
    "    a = torch.mean(A, dim=[2, 3], keepdim=True)*0.6\n",
    "    M = (A > a).float()\n",
    "\n",
    "    A1 = torch.sum(fm1, dim=1, keepdim=True)\n",
    "    a1 = torch.mean(A1, dim=[2, 3], keepdim=True)*0.6\n",
    "    M1 = (A1 > a1).float()\n",
    "\n",
    "\n",
    "    coordinates = []\n",
    "    for i, m in enumerate(M):\n",
    "        mask_np = m.cpu().numpy().reshape(14, 14)\n",
    "        component_labels = measure.label(mask_np, connectivity=2)\n",
    "\n",
    "        properties = measure.regionprops(component_labels)\n",
    "        areas = []\n",
    "        for prop in properties:\n",
    "            areas.append(prop.area)\n",
    "#         print(areas)\n",
    "        max_idx = areas.index(max(areas))\n",
    "\n",
    "\n",
    "        intersection = ((component_labels==(max_idx+1)).astype(int) + (M1[i][0].cpu().numpy()==1).astype(int)) ==2\n",
    "        prop = measure.regionprops(intersection.astype(int))\n",
    "        if len(prop) == 0:\n",
    "            bbox = [0, 0, 14, 14]\n",
    "#             print('there is one img no intersection')\n",
    "        else:\n",
    "            bbox = prop[0].bbox\n",
    "#             print(\"B box is\",bbox)\n",
    "        x_lefttop = bbox[0] * 32 - 1\n",
    "        y_lefttop = bbox[1] * 32 - 1\n",
    "        x_rightlow = bbox[2] * 32 - 1\n",
    "        y_rightlow = bbox[3] * 32 - 1\n",
    "        # for image\n",
    "        if x_lefttop < 0:\n",
    "            x_lefttop = 0\n",
    "        if y_lefttop < 0:\n",
    "            y_lefttop = 0\n",
    "        coordinate = [x_lefttop, y_lefttop, x_rightlow, y_rightlow]\n",
    "        coordinates.append(coordinate)\n",
    "    return coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11ef31a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# segmentation function\n",
    "##################################\n",
    "def batch_segmentation(images, attention_map, mode='crop', theta=0.5, padding_ratio=0.1):\n",
    "    batches, _, imgH, imgW = images.size()\n",
    "\n",
    "    if mode == 'crop':\n",
    "        crop_images = []\n",
    "        for batch_index in range(batches):\n",
    "            atten_map = attention_map[batch_index:batch_index + 1]\n",
    "            if isinstance(theta, tuple):\n",
    "                theta_c = random.uniform(*theta) * atten_map.max()\n",
    "            else:\n",
    "                theta_c = theta * atten_map.max()\n",
    "\n",
    "            crop_mask = F.upsample_bilinear(atten_map, size=(imgH, imgW)) >= theta_c\n",
    "            nonzero_indices = torch.nonzero(crop_mask[0, 0, ...])\n",
    "            height_min = max(int(nonzero_indices[:, 0].min().item() - padding_ratio * imgH), 0)\n",
    "            height_max = min(int(nonzero_indices[:, 0].max().item() + padding_ratio * imgH), imgH)\n",
    "            width_min = max(int(nonzero_indices[:, 1].min().item() - padding_ratio * imgW), 0)\n",
    "            width_max = min(int(nonzero_indices[:, 1].max().item() + padding_ratio * imgW), imgW)\n",
    "\n",
    "            crop_images.append(\n",
    "                F.upsample_bilinear(images[batch_index:batch_index + 1, :, height_min:height_max, width_min:width_max],\n",
    "                                    size=(imgH, imgW)))\n",
    "        crop_images = torch.cat(crop_images, dim=0)\n",
    "        return crop_images\n",
    "\n",
    "    elif mode == 'drop':\n",
    "        drop_masks = []\n",
    "        for batch_index in range(batches):\n",
    "            atten_map = attention_map[batch_index:batch_index + 1]\n",
    "            if isinstance(theta, tuple):\n",
    "                theta_d = random.uniform(*theta) * atten_map.max()\n",
    "            else:\n",
    "                theta_d = theta * atten_map.max()\n",
    "            msk=F.upsample_bilinear(atten_map, size=(imgH, imgW)) < theta_d\n",
    "            q =msk.float()\n",
    "            tense_tensor = torch.tensor(q, device='cuda')\n",
    "            label=torch.where(tense_tensor==0.0, torch.tensor(2.0, device = 'cuda'), tense_tensor)\n",
    "#             drop_masks.append(label)\n",
    "            drop_masks.append(label2rgb(label, images[batch_index], kind = 'overlay'))\n",
    "        drop_masks = torch.cat(drop_masks, dim=0).float()\n",
    "#         label2rgb(astronaut_segments, image, kind = 'overlay')\n",
    "#         drop_images = images * drop_masks.float()\n",
    "        return drop_masks\n",
    "    else:\n",
    "        raise ValueError('Expected mode in [\\'crop\\', \\'drop\\'], but received unsupported augmentation method %s' % mode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b29e78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2689195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "##################################\n",
    "# augment function\n",
    "##################################\n",
    "def batch_augment(images, attention_map, mode='drop', theta=0.4, padding_ratio=0.1, red_p=0.9):\n",
    "    batches, _, imgH, imgW = images.size()\n",
    "\n",
    "    if mode == 'crop':\n",
    "        crop_images = []\n",
    "        for batch_index in range(batches):\n",
    "            atten_map = attention_map[batch_index:batch_index + 1]\n",
    "            if isinstance(theta, tuple):\n",
    "                theta_c = random.uniform(*theta) * atten_map.max()\n",
    "            else:\n",
    "                theta_c = theta * atten_map.max()\n",
    "\n",
    "            crop_mask = F.upsample_bilinear(atten_map, size=(imgH, imgW)) >= theta_c\n",
    "            nonzero_indices = torch.nonzero(crop_mask[0, 0, ...])\n",
    "            height_min = max(int(nonzero_indices[:, 0].min().item() - padding_ratio * imgH), 0)\n",
    "            height_max = min(int(nonzero_indices[:, 0].max().item() + padding_ratio * imgH), imgH)\n",
    "            width_min = max(int(nonzero_indices[:, 1].min().item() - padding_ratio * imgW), 0)\n",
    "            width_max = min(int(nonzero_indices[:, 1].max().item() + padding_ratio * imgW), imgW)\n",
    "\n",
    "            crop_images.append(\n",
    "                F.upsample_bilinear(images[batch_index:batch_index + 1, :, height_min:height_max, width_min:width_max],\n",
    "                                    size=(imgH, imgW)))\n",
    "        crop_images = torch.cat(crop_images, dim=0)\n",
    "        return crop_images\n",
    "\n",
    "    elif mode == 'drop':\n",
    "        drop_masks = []\n",
    "        for batch_index in range(batches):\n",
    "            atten_map = attention_map[batch_index:batch_index + 1]\n",
    "            if isinstance(theta, tuple):\n",
    "                theta_d = random.uniform(*theta) * atten_map.max()\n",
    "            else:\n",
    "                theta_d = theta * atten_map.max()\n",
    "            msk=F.upsample_bilinear(atten_map, size=(imgH, imgW)) < theta_d\n",
    "            q =msk.float()\n",
    "            tense_tensor = torch.tensor(q, device='cuda')\n",
    "            drop_masks.append(torch.where(tense_tensor==0.0, torch.tensor(red_p, device = 'cuda'), tense_tensor))\n",
    "        drop_masks = torch.cat(drop_masks, dim=0)\n",
    "        drop_images = images * drop_masks\n",
    "        return drop_images\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Expected mode in [\\'crop\\', \\'drop\\'], but received unsupported augmentation method %s' % mode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5dbf904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=torch.rand(5, 3, 448, 448)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "415b616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod=resnet50(pretrained=True, pth_path=pretrain_path_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "632c8671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modf1=MyEnsemble(SSPCAB(3, reduction_ratio=8), resnet50(pretrained=True, pth_path=pretrain_path_B))\n",
    "# modf2=MyEnsemble(SSPCAB(3, reduction_ratio=4), resnet50(pretrained=True, pth_path=pretrain_path_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3fbe685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a, b= mod(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a873672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8acf515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod = xception(pretrained=True)\n",
    "# iou_threshs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e6abf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from networks import Axial_Nets\n",
    "from networks import resnet\n",
    "from config import pretrain_path, coordinates_cat, iou_threshs, window_nums_sum, ratios, N_list\n",
    "import numpy as np\n",
    "# from utils.AOLM import AOLM\n",
    "# pretrain_path_B = './models/pretrained/xception-c0a72b38.pth.tar'\n",
    "pretrain_path_B = './models/pretrained/resnet50-19c8e357.pth'\n",
    "def nms(scores_np, proposalN, iou_threshs, coordinates):\n",
    "    if not (type(scores_np).__module__ == 'numpy' and len(scores_np.shape) == 2 and scores_np.shape[1] == 1):\n",
    "        raise TypeError('score_np is not right')\n",
    "\n",
    "    windows_num = scores_np.shape[0]\n",
    "    indices_coordinates = np.concatenate((scores_np, coordinates), 1)\n",
    "\n",
    "    indices = np.argsort(indices_coordinates[:, 0])\n",
    "    indices_coordinates = np.concatenate((indices_coordinates, np.arange(0,windows_num).reshape(windows_num,1)), 1)[indices]                  #[339,6]\n",
    "    indices_results = []\n",
    "\n",
    "    res = indices_coordinates\n",
    "\n",
    "    while res.any():\n",
    "        indice_coordinates = res[-1]\n",
    "        indices_results.append(indice_coordinates[5])\n",
    "\n",
    "        if len(indices_results) == proposalN:\n",
    "            return np.array(indices_results).reshape(1,proposalN).astype(np.int64)\n",
    "        res = res[:-1]\n",
    "\n",
    "        # Exclude anchor boxes with selected anchor box whose iou is greater than the threshold\n",
    "        start_max = np.maximum(res[:, 1:3], indice_coordinates[1:3])\n",
    "        end_min = np.minimum(res[:, 3:5], indice_coordinates[3:5])\n",
    "        lengths = end_min - start_max + 1\n",
    "        intersec_map = lengths[:, 0] * lengths[:, 1]\n",
    "        intersec_map[np.logical_or(lengths[:, 0] < 0, lengths[:, 1] < 0)] = 0\n",
    "        iou_map_cur = intersec_map / ((res[:, 3] - res[:, 1] + 1) * (res[:, 4] - res[:, 2] + 1) +\n",
    "                                      (indice_coordinates[3] - indice_coordinates[1] + 1) *\n",
    "                                      (indice_coordinates[4] - indice_coordinates[2] + 1) - intersec_map)\n",
    "        res = res[iou_map_cur <= iou_threshs]\n",
    "\n",
    "    while len(indices_results) != proposalN:\n",
    "        indices_results.append(indice_coordinates[5])\n",
    "\n",
    "    return np.array(indices_results).reshape(1, -1).astype(np.int64)\n",
    "\n",
    "class APPM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(APPM, self).__init__()\n",
    "        self.avgpools = [nn.AvgPool2d(ratios[i], 1) for i in range(len(ratios))]\n",
    "\n",
    "    def forward(self, proposalN, x, ratios, window_nums_sum, N_list, iou_threshs, DEVICE='cuda'):\n",
    "        batch, channels, _, _ = x.size()\n",
    "        avgs = [self.avgpools[i](x) for i in range(len(ratios))]\n",
    "\n",
    "        # feature map sum\n",
    "        fm_sum = [torch.sum(avgs[i], dim=1) for i in range(len(ratios))]\n",
    "\n",
    "        all_scores = torch.cat([fm_sum[i].view(batch, -1, 1) for i in range(len(ratios))], dim=1)\n",
    "        windows_scores_np = all_scores.data.cpu().numpy()\n",
    "        window_scores = torch.from_numpy(windows_scores_np).to(DEVICE).reshape(batch, -1)\n",
    "\n",
    "        # nms\n",
    "        proposalN_indices = []\n",
    "        for i, scores in enumerate(windows_scores_np):\n",
    "            indices_results = []\n",
    "            for j in range(len(window_nums_sum)-1):\n",
    "                indices_results.append(nms(scores[sum(window_nums_sum[:j+1]):sum(window_nums_sum[:j+2])], proposalN=N_list[j], iou_threshs=iou_threshs[j],\n",
    "                                           coordinates=coordinates_cat[sum(window_nums_sum[:j+1]):sum(window_nums_sum[:j+2])]) + sum(window_nums_sum[:j+1]))\n",
    "            # indices_results.reverse()\n",
    "            proposalN_indices.append(np.concatenate(indices_results, 1))   # reverse\n",
    "\n",
    "        proposalN_indices = np.array(proposalN_indices).reshape(batch, proposalN)\n",
    "        proposalN_indices = torch.from_numpy(proposalN_indices).to(DEVICE)\n",
    "        proposalN_windows_scores = torch.cat(\n",
    "            [torch.index_select(all_score, dim=0, index=proposalN_indices[i]) for i, all_score in enumerate(all_scores)], 0).reshape(\n",
    "            batch, proposalN)\n",
    "\n",
    "        return proposalN_indices, proposalN_windows_scores, window_scores\n",
    "\n",
    "class MainNet(nn.Module):\n",
    "    def __init__(self, proposalN, num_classes, channels, theta, red_p):\n",
    "       \n",
    "        super(MainNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.proposalN = proposalN\n",
    "        self.M=3\n",
    "        self.theta=theta\n",
    "        self.red_p = red_p\n",
    "        self.SSPCAB_Block = SSPCAB(2048)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "#         self.pretrained_model =Net()\n",
    "#         self.pretrained_model = axial50s(k=56)\n",
    "\n",
    "#         self.pretrained_model_A = resnet.resnet50(pretrained=True, pth_path=pretrain_path)\n",
    "        self.pretrained_model_B = resnet50(pretrained=True, pth_path=pretrain_path_B)\n",
    "#         self.pretrained_model_B = xception(pretrained=True, pth_path=pretrain_path_B)\n",
    "        self.rawcls_net = nn.Linear(channels, num_classes)\n",
    "        self.APPM = APPM()\n",
    "# (self, layers, output_dim, heads, input_resolution=448, width=64)\n",
    "    def forward(self, x, epoch, batch_idx, status='test', DEVICE='cuda'):\n",
    "#         print(x.shape)\n",
    "        att_out=x\n",
    "#         att_out=batch_augment(x, torch.sum(self.pretrained_model_A(x)[-1], dim=1, keepdim=True),\n",
    "#                               mode='drop', theta=self.theta, red_p=self.red_p)\n",
    "#         print(att_out.shape)\n",
    "        fm1 = self.pretrained_model_B(att_out)\n",
    "        fm2 = self.SSPCAB_Block(fm1)\n",
    "        embeeding = self.avgpool(fm2)\n",
    "        embeeding = embeeding.view(embeeding.size(0), -1)\n",
    "        embeeding = self.dropout(embeeding)\n",
    "        batch_size, channel_size, side_size, _ = fm2.shape\n",
    "#         assert channel_size == 2048\n",
    "        \n",
    "\n",
    "        # raw branch\n",
    "        raw_logits = self.rawcls_net(embeeding)\n",
    "        \n",
    "#         print(\"raw_logits\",raw_logits)\n",
    "#         y=raw_logits.argmax(dim=-1)\n",
    "\n",
    "        #SCDA\n",
    "#         coordinats=torch.tensor(TSCM(tscams,y,x,batch_size))\n",
    "        coordinates = torch.tensor(AOLM(fm1.detach(), fm2.detach()))\n",
    "#         print(\"coordinate\",coordinates)\n",
    "\n",
    "        local_imgs = torch.zeros([batch_size, 3, 448, 448]).to(DEVICE)  # [N, 3, 448, 448]\n",
    "        for i in range(batch_size):\n",
    "            [x0, y0, x1, y1] = coordinates[i]\n",
    "            local_imgs[i:i + 1] = F.interpolate(att_out[i:i + 1, :, x0:(x1+1), y0:(y1+1)], size=(448, 448),\n",
    "                                                mode='bilinear', align_corners=True)  # [N, 3, 224, 224]\n",
    "        local_fm1 = self.pretrained_model_B(local_imgs.detach())  # [N, 2048]\n",
    "        local_fm = self.SSPCAB_Block(local_fm1)\n",
    "        local_embeddings = self.avgpool(local_fm)\n",
    "        local_embeddings = local_embeddings.view(local_embeddings.size(0), -1)\n",
    "        local_embeddings = self.dropout(local_embeddings)\n",
    "        local_logits = self.rawcls_net(local_embeddings)  # [N, 2]\n",
    "        #To find final Local image consist of object\n",
    "        \n",
    "        \n",
    "#         final_coordinates = torch.tensor(AOLM(fm.detach(), tscams.detach()))\n",
    "#         final_local_imgs = torch.zeros([batch_size, 3, 448, 448]).to(DEVICE)  # [N, 3, 448, 448]\n",
    "#         for i in range(batch_size):\n",
    "#             [x0, y0, x1, y1] = final_coordinates[i]\n",
    "#             final_local_imgs[i:i + 1] = F.interpolate(att_out[i:i + 1, :, x0:(x1+1), y0:(y1+1)], size=(448, 448),\n",
    "#                                                 mode='bilinear', align_corners=True)  # [N, 3, 224, 224]\n",
    "#         final_local_fm, final_local_embeddings, _, _ = self.pretrained_model_B(final_local_imgs.detach())  # [N, 2048]\n",
    "#         final_local_logits = self.rawcls_net(final_local_embeddings)  # [N, 2]\n",
    "        proposalN_indices, proposalN_windows_scores, window_scores \\\n",
    "            = self.APPM(self.proposalN, local_fm.detach(), ratios, window_nums_sum, N_list, iou_threshs, DEVICE)\n",
    "\n",
    "        if status == \"train\":\n",
    "            # window_imgs cls\n",
    "            window_imgs = torch.zeros([batch_size, self.proposalN, 3, 224, 224]).to(DEVICE)  # [N, 4, 3, 224, 224]\n",
    "            wnds=[]\n",
    "            for i in range(batch_size):\n",
    "                wnd=[]\n",
    "                for j in range(self.proposalN):\n",
    "                    [x0, y0, x1, y1] = coordinates_cat[proposalN_indices[i, j]]\n",
    "                    window_imgs[i:i + 1, j] = F.interpolate(local_imgs[i:i + 1, :, x0:(x1 + 1), y0:(y1 + 1)], size=(224, 224),\n",
    "                                                                mode='bilinear',\n",
    "                                                                align_corners=True)  # [N, 4, 3, 224, 224]\n",
    "                    wnd.append([x0, y0, x1, y1])\n",
    "                wnds.append(wnd)\n",
    "                   \n",
    "            window_imgs = window_imgs.reshape(batch_size * self.proposalN, 3, 224, 224)  # [N*4, 3, 224, 224]\n",
    "            window_fm = self.pretrained_model_B(window_imgs.detach())  # [N*4, 2048]\n",
    "            window_fm = self.SSPCAB_Block(window_fm)\n",
    "            window_fm = self.avgpool(window_fm)\n",
    "            window_fm = window_fm.view(window_fm.size(0), -1)\n",
    "            window_embeddings = self.dropout(window_fm)         \n",
    " \n",
    "            proposalN_windows_logits = self.rawcls_net(window_embeddings)  # [N* 4, 200]\n",
    "        else:\n",
    "            proposalN_windows_logits = torch.zeros([batch_size * self.proposalN, self.num_classes]).to(DEVICE)\n",
    "\n",
    "        return proposalN_windows_scores, proposalN_windows_logits, proposalN_indices, \\\n",
    "    window_scores, coordinates, raw_logits, local_logits, local_imgs, att_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1a302cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_imgs(org, ot, codd):\n",
    "    size=(codd[3]-codd[1], codd[2]-codd[0])\n",
    "    image_boxes2 = Image.fromarray(ot)\n",
    "    image_boxes2 = image_boxes2.resize(size)\n",
    "    imgo = Image.fromarray(org)\n",
    "    imgo.paste(image_boxes2, (codd[1], codd[0]))\n",
    "    return imgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666364d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "63080361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "from torchmetrics import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bea1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6128413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import CohenKappa\n",
    "from torchmetrics import ConfusionMatrix\n",
    "from torchmetrics.functional import auc\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def classification_results(pre, tar, stat):\n",
    "    confmat = ConfusionMatrix(num_classes=2)\n",
    "    cohenkappa = CohenKappa(num_classes=2)\n",
    "    acc=Accuracy()\n",
    "    \n",
    "    print(\"*****************************\")\n",
    "    print(stat)\n",
    "    print(\"Confusion Matrix\", confmat(tar, pre).numpy())\n",
    "#     print(\"cohenkappa score\", cohenkappa(tar, pre).numpy())\n",
    "#     print(\"AUC\", roc_auc_score(tar, pre))\n",
    "#     print(\"accuracy\", acc(tar, pre).numpy())\n",
    "#     print(\"MCC\", matthews_corrcoef(tar, pre))\n",
    "    print(\"*****************************\")\n",
    "    return cohenkappa(tar, pre).numpy(), roc_auc_score(tar, pre), matthews_corrcoef(tar, pre)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a1481e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c6b4d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83895281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b3f689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs, dataloaders, dataset_sizes):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        p=[]\n",
    "        q=[]\n",
    "        for phase in ['train', 'val']:\n",
    "            \n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "#                 inputs, labels= data\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)[-1]\n",
    "                    #              output = model(input)\n",
    "                    if isinstance(outputs, tuple): # <-- inception output is a tuple (x, aux)\n",
    "                        outputs = outputs[0]  \n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "#                     print(preds)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "#                 print(preds)\n",
    "#                 print((labels.data))\n",
    "                p.append(preds)\n",
    "                q.append(labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            Q=torch.cat(q).reshape(-1).cpu()\n",
    "            P=torch.cat(p).reshape(-1).cpu()\n",
    "#             print(P)\n",
    "#             print(Q)\n",
    "            classification_results(P, Q, phase)\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "#         print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "#     model.load_state_dict(best_model_wts)\n",
    "    return best_model_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97803722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "from config import coordinates_cat, proposalN, set, vis_num\n",
    "from utils.cal_iou import calculate_iou\n",
    "from utils.vis import image_with_boxes, image_with_box\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "CFR=True\n",
    "def eval(model, testloader, criterion, status, save_path, epoch):\n",
    "    model.eval()\n",
    "    print('Evaluating')\n",
    "\n",
    "    raw_loss_sum = 0\n",
    "    local_loss_sum = 0\n",
    "#     final_local_loss_sum=0\n",
    "    windowscls_loss_sum = 0\n",
    "    total_loss_sum = 0\n",
    "    iou_corrects = 0\n",
    "    raw_correct = 0\n",
    "    local_correct = 0\n",
    "#     final_local_correct=0\n",
    "    obtain_row=[]\n",
    "#     obtain_final_local=[]\n",
    "    obtain_local=[]\n",
    "    desire=[]\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(tqdm(testloader)):\n",
    "            if set == 'CUB':\n",
    "                images, labels, boxes, scale = data\n",
    "            else:\n",
    "                images, labels = data\n",
    "            desire.append(labels)\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            proposalN_windows_score,proposalN_windows_logits, indices, \\\n",
    "            window_scores, coordinates, raw_logits, local_logits, local_imgs, \\\n",
    "            att_out = model(images, epoch, i, status)\n",
    "            raw_loss = criterion(raw_logits, labels)\n",
    "            local_loss = criterion(local_logits, labels)\n",
    "#             final_local_loss = criterion(final_local_logits,labels)\n",
    "            windowscls_loss = criterion(proposalN_windows_logits,\n",
    "                                        labels.unsqueeze(1).repeat(1, proposalN).view(-1))\n",
    "\n",
    "            total_loss = raw_loss + local_loss + windowscls_loss\n",
    "\n",
    "            raw_loss_sum += raw_loss.item()\n",
    "            local_loss_sum += local_loss.item()\n",
    "#             final_local_loss_sum += final_local_loss.item()\n",
    "            windowscls_loss_sum += windowscls_loss.item()\n",
    "\n",
    "            total_loss_sum += total_loss.item()\n",
    "            if set == 'CUB':\n",
    "                # computer resized coordinates of boxes\n",
    "                boxes_coor = boxes.float()\n",
    "                resized_boxes = torch.cat([(boxes_coor[:,0] * scale[:, 0]).unsqueeze(1) ,(boxes_coor[:,1] * scale[:, 1]).unsqueeze(1),\n",
    "                                           (boxes_coor[:,2] * scale[:, 0]).unsqueeze(1), (boxes_coor[:,3] * scale[:, 1]).unsqueeze(1)], dim=1)\n",
    "                resized_coor = torch.cat([resized_boxes[:,0].unsqueeze(1) ,resized_boxes[:,1].unsqueeze(1),\n",
    "                                           (resized_boxes[:,0] + resized_boxes[:,2]).unsqueeze(1), (resized_boxes[:,1]+resized_boxes[:,3]).unsqueeze(1)], dim=1).round().int()\n",
    "\n",
    "\n",
    "                iou = calculate_iou(coordinates.cpu().numpy(), resized_coor.numpy())\n",
    "#                 print(iou)\n",
    "                iou_corrects += np.sum(iou >= 0.5)\n",
    "#                 print(iou_corrects)\n",
    "#                 with SummaryWriter(log_dir=os.path.join(save_path, 'log'), comment='test') as writer:\n",
    "#                     writer.add_scalar('Test/PCP', iou_corrects, epoch+i)\n",
    "            # Row branch\n",
    "\n",
    "            pred = raw_logits.max(1, keepdim=True)[1]\n",
    "            raw_correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "            obtain_row.append(pred)\n",
    "            # local branch\n",
    "            pred = local_logits.max(1, keepdim=True)[1]\n",
    "            obtain_local.append(pred)\n",
    "            local_correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "            if i == 0:\n",
    "                if set == 'CUB':\n",
    "                    box_coor = resized_coor[:vis_num].numpy()\n",
    "                    pred_coor = coordinates[:vis_num].cpu().numpy()\n",
    "                    with SummaryWriter(log_dir=os.path.join(save_path, 'log'), comment=status + 'raw') as writer:\n",
    "                        cat_imgs = []\n",
    "                        for j, coor in enumerate(box_coor):\n",
    "                            img = image_with_boxes(images[j], [coor])\n",
    "                            img = image_with_boxes(img, [pred_coor[j]], color=(0, 255, 0))\n",
    "                            cat_imgs.append(img)\n",
    "                        cat_imgs = np.concatenate(cat_imgs, axis=1)\n",
    "                        writer.add_images(status + '/' + 'raw image with boxes', cat_imgs, epoch, dataformats='HWC')\n",
    "\n",
    "            \n",
    "#             #final_local\n",
    "#             pred = final_local_logits.max(1, keepdim=True)[1]\n",
    "#             obtain_final_local.append(pred)\n",
    "#             final_local_correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "            # object branch tensorboard\n",
    "            indices_ndarray = indices[:vis_num,:proposalN].cpu().numpy()\n",
    "            if i==0:\n",
    "#             or i== 1 or i==2:\n",
    "#                 or i== 3 or i==4:\n",
    "                with SummaryWriter(log_dir=os.path.join(save_path, 'log'), comment=status + 'Final Results') as writer:\n",
    "                    cat_imgs = []\n",
    "                    no_box_imgs=[]\n",
    "                    local_ims=[]\n",
    "#                     final_local_ims=[]\n",
    "                    s_box_imgs=[]\n",
    "                    for j, indice_ndarray in enumerate(indices_ndarray):\n",
    "\n",
    "#                         if labels[j]==10:\n",
    "                        results=[]\n",
    "                        att=image_with_boxes(att_out[j])\n",
    "                        im = image_with_boxes(images[j])\n",
    "                        results.append(im)\n",
    "                        results.append(att)\n",
    "                        local_im = image_with_boxes(local_imgs[j])\n",
    "                        results.append(local_im)\n",
    "#                             final_local_im = image_with_boxes(final_local_imgs[j])\n",
    "#                             results.append(final_local_im)\n",
    "                        img = image_with_boxes(local_imgs[j], coordinates_cat[indice_ndarray])\n",
    "                        results.append(img)\n",
    "                        s_box_img=image_with_box(local_imgs[j], coordinates_cat[indice_ndarray])\n",
    "                        results.append(s_box_img)\n",
    "                        fin_res=combine_imgs(im, s_box_img, coordinates[j])\n",
    "                        results.append(fin_res)\n",
    "#                         print(results)\n",
    "                        results = np.concatenate(results, axis=1)\n",
    "#                         show_images(results, cols = 1)\n",
    "\n",
    "                        writer.add_images(status + '/' + 'Original images' +'/' + 'Local images'+ '/' +\n",
    "                                          'Object image with windows'+ str(i)+str(j), results, epoch, dataformats='HWC')\n",
    "\n",
    "    raw_loss_avg = raw_loss_sum / (i+1)\n",
    "    local_loss_avg = local_loss_sum / (i+1)\n",
    "#     final_local_loss_avg = final_local_loss_sum / (i+1)\n",
    "    windowscls_loss_avg = windowscls_loss_sum / (i+1)\n",
    "    total_loss_avg = total_loss_sum / (i+1)\n",
    "\n",
    "    raw_accuracy = raw_correct / len(testloader.dataset)\n",
    "    local_accuracy = local_correct / len(testloader.dataset)\n",
    "    PCP = iou_corrects / len(testloader.dataset)\n",
    "#     print(PCP)\n",
    "\n",
    "    \n",
    "#     if CFR==True:\n",
    "#         tar=torch.cat(desire).reshape(-1).cpu()\n",
    "#         o_r=torch.cat(obtain_row).reshape(-1).cpu()\n",
    "# #         o_fl=torch.cat(obtain_final_local).reshape(-1).cpu()\n",
    "#         o_l=torch.cat(obtain_local).reshape(-1).cpu()\n",
    "# #         _, _, _=classification_results(o_r, tar, \"Row\")\n",
    "# #         classification_results(o_fl, tar, \"Final_Local\")\n",
    "#         print(o_r.shape, o_l.shape)\n",
    "#         C, AC, MAT=classification_results(o_l, tar, \"Local\")\n",
    "    return raw_loss_avg, windowscls_loss_avg, total_loss_avg, raw_accuracy, local_accuracy, local_loss_avg, PCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "caf6c28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MainNet(proposalN=proposalN, num_classes=num_classes, channels=2048, theta=(0.6), red_p=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aa4dc39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test=torch.rand(5, 3, 448,448)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "228677a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test=model.modf1.modelB(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc41e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f565e523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7e12fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "74e3d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "from config import max_checkpoint_num, proposalN, eval_trainset, set\n",
    "# from utils.eval_model import eval\n",
    "\n",
    "def train(model,\n",
    "          trainloader,\n",
    "          testloader,\n",
    "          criterion,\n",
    "          optimizer,\n",
    "          scheduler,\n",
    "          save_path,\n",
    "          start_epoch,\n",
    "          end_epoch,\n",
    "          num_epochs,\n",
    "          save_interval):\n",
    "    eval_trainset=False\n",
    "#     dataset_sizes = {['train', 'val'][x]: [len(trainloader.dataset),len(testloader.dataset)][x] for x in range(len(['train', 'val']))}\n",
    "    \n",
    "#     dataloaders = {['train', 'val'][x]: [trainloader,testloader][x] for x in range(len(['train', 'val']))}\n",
    "    \n",
    "#     print(\"Exploring...\")\n",
    "#     if start_epoch < 2:\n",
    "#         best_model_wts = train_model(model.pretrained_model_B, criterion, optimizer, scheduler, num_epochs,\n",
    "#                                            dataloaders, dataset_sizes)\n",
    "#         model.pretrained_model_B.load_state_dict(best_model_wts)\n",
    "#     print(\"Expolaration done.\")\n",
    "    for epoch in range(start_epoch + 1, end_epoch + 1):\n",
    "        model.train()\n",
    "        \n",
    "        print('Training %d epoch' % epoch)\n",
    "        \n",
    "        lr = next(iter(optimizer.param_groups))['lr']\n",
    "\n",
    "        for i, data in enumerate(tqdm(trainloader, 0)):\n",
    "            if set == 'CUB':\n",
    "                images, labels, _, _ = data\n",
    "            else:\n",
    "                images, labels = data\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            proposalN_windows_score, proposalN_windows_logits, indices, \\\n",
    "            window_scores, _, raw_logits, local_logits, local_imgs, _ = model(images, epoch, i, 'train')\n",
    "\n",
    "            raw_loss = criterion(raw_logits, labels)\n",
    "            local_loss = criterion(local_logits, labels)\n",
    "#             final_local_loss=criterion(final_local_logits, labels)\n",
    "            windowscls_loss = criterion(proposalN_windows_logits,\n",
    "                               labels.unsqueeze(1).repeat(1, proposalN).view(-1))\n",
    "\n",
    "            if epoch < 2:\n",
    "                total_loss = raw_loss\n",
    "            else:\n",
    "                total_loss = raw_loss + local_loss + windowscls_loss\n",
    "\n",
    "            total_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "#         evaluation every epoch\n",
    "        if eval_trainset:\n",
    "            raw_loss_avg, windowscls_loss_avg, total_loss_avg, raw_accuracy, local_accuracy, local_loss_avg, PCP = eval(model, trainloader, criterion, 'train', save_path, epoch)\n",
    "            \n",
    "\n",
    "            print(\n",
    "                'Train set: raw accuracy: {:.2f}%, local accuracy: {:.2f}%'.format(100. * raw_accuracy, 100. * local_accuracy))\n",
    "\n",
    "            # tensorboard\n",
    "            with SummaryWriter(log_dir=os.path.join(save_path, 'log'), comment='train') as writer:\n",
    "\n",
    "                writer.add_scalar('Train/learning rate', lr, epoch)\n",
    "                writer.add_scalar('Train/raw_accuracy', raw_accuracy, epoch)\n",
    "                writer.add_scalar('Train/local_accuracy', local_accuracy, epoch)\n",
    "#                 writer.add_scalar('Train/CohenKappa', C, epoch)\n",
    "                writer.add_scalar('Train/raw_loss_avg', raw_loss_avg, epoch)\n",
    "                writer.add_scalar('Train/local_loss_avg', local_loss_avg, epoch)\n",
    "#                 writer.add_scalar('Train/AUC', AC, epoch)\n",
    "#                 writer.add_scalar('Train/MCC', MAT, epoch)\n",
    "                writer.add_scalar('Train/windowscls_loss_avg', windowscls_loss_avg, epoch)\n",
    "                writer.add_scalar('Train/total_loss_avg', total_loss_avg, epoch)\n",
    "                \n",
    "\n",
    "        # eval testset\n",
    "        raw_loss_avg, windowscls_loss_avg, total_loss_avg, raw_accuracy, local_accuracy, local_loss_avg, PCP = eval(model, testloader, criterion, 'test', save_path, epoch)\n",
    "        print('Test set: raw accuracy: {:.2f}%, local accuracy: {:.2f}%, PCP: {:.2f}%'.format(100. * raw_accuracy, 100. * local_accuracy, PCP))\n",
    "\n",
    "        # tensorboard\n",
    "        with SummaryWriter(log_dir=os.path.join(save_path, 'log'), comment='test') as writer:\n",
    "            writer.add_scalar('Test/PCP', PCP, epoch)\n",
    "            writer.add_scalar('Test/raw_accuracy', raw_accuracy, epoch)\n",
    "            writer.add_scalar('Test/local_accuracy', local_accuracy, epoch)\n",
    "#             writer.add_scalar('Test/CohenKappa', C, epoch)\n",
    "            writer.add_scalar('Test/raw_loss_avg', raw_loss_avg, epoch)\n",
    "            writer.add_scalar('Test/local_loss_avg', local_loss_avg, epoch)\n",
    "#             writer.add_scalar('Test/AUC', AC, epoch)\n",
    "#             writer.add_scalar('Test/MCC', MAT, epoch)\n",
    "            writer.add_scalar('Test/windowscls_loss_avg', windowscls_loss_avg, epoch)\n",
    "            writer.add_scalar('Test/total_loss_avg', total_loss_avg, epoch)\n",
    "\n",
    "        # save checkpoint\n",
    "        if (epoch % save_interval == 0) or (epoch == end_epoch):\n",
    "            print('Saving checkpoint')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'learning_rate': lr,\n",
    "            }, os.path.join(save_path, 'epoch' + str(epoch) + '.pth'))\n",
    "\n",
    "        # Limit the number of checkpoints to less than or equal to max_checkpoint_num,\n",
    "        # and delete the redundant ones\n",
    "        checkpoint_list = [os.path.basename(path) for path in glob.glob(os.path.join(save_path, '*.pth'))]\n",
    "        if len(checkpoint_list) == max_checkpoint_num + 1:\n",
    "            idx_list = [int(name.replace('epoch', '').replace('.pth', '')) for name in checkpoint_list]\n",
    "            min_idx = min(idx_list)\n",
    "            os.remove(os.path.join(save_path, 'epoch' + str(min_idx) + '.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5083452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image as im\n",
    "def tenssor2array(images):\n",
    "    if type(images) is not np.ndarray:\n",
    "        image = images.clone().detach()\n",
    "\n",
    "        rgbN = [(255, 0, 0), (255, 165, 0), (255, 255, 0), (0, 255, 0), (0, 255, 0), (0, 255, 0), (0, 255, 0)]\n",
    "\n",
    "        # Anti-normalization\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        image[0] = image[0] * std[0] + mean[0]\n",
    "        image[1] = image[1] * std[1] + mean[1]\n",
    "        image[2] = image[2].mul(std[2]) + mean[2]\n",
    "        image = image.mul(255).byte()\n",
    "\n",
    "        image = image.data.cpu().numpy()\n",
    "\n",
    "        image.astype(np.uint8)\n",
    "\n",
    "        image = np.transpose(image, (1, 2, 0))  # CHW --> HWC\n",
    "        \n",
    "    return im.fromarray(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a019599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.8\n",
    "GAMMA = 2\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, alpha=ALPHA, gamma=GAMMA, smooth=1):\n",
    "\n",
    "        \n",
    "        #first compute binary cross-entropy \n",
    "        BCE = nn.CrossEntropyLoss()(inputs, targets)\n",
    "        BCE_EXP = torch.exp(-BCE)\n",
    "        focal_loss = alpha * (1-BCE_EXP)**gamma * BCE\n",
    "                       \n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c11612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "163695f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "53f0167d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "346e34f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cce4b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "567d4bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TUMOR():\n",
    "    def __init__(self, input_size, root, is_train=True, data_len=None):\n",
    "        self.input_size = input_size\n",
    "        self.root = root\n",
    "#         self.transform = transform\n",
    "#         self.to_pil = transforms.ToPILImage()\n",
    "        self.is_train = is_train\n",
    "        train_img_path = os.path.join(self.root)\n",
    "        test_img_path = os.path.join(self.root)\n",
    "        train_label_file = open(os.path.join(self.root, 'train.txt'))\n",
    "        test_label_file = open(os.path.join(self.root, 'test.txt'))\n",
    "        train_img_label = []\n",
    "        test_img_label = []\n",
    "        for line in train_label_file:\n",
    "            train_img_label.append([os.path.join(train_img_path, line[:-2]), int(line[-2])])\n",
    "        for line in test_label_file:\n",
    "            test_img_label.append([os.path.join(test_img_path, line[:-2]), int(line[-2])])\n",
    "        self.train_img_label = train_img_label[:data_len]\n",
    "        self.test_img_label = test_img_label[:data_len]\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.is_train:\n",
    "            img, target = imageio.imread(self.train_img_label[index][0]), self.train_img_label[index][1]\n",
    "            if len(img.shape) == 2:\n",
    "                img = np.stack([img] * 3, 2)\n",
    "            img = Image.fromarray(img, mode='RGB')\n",
    "\n",
    "            img = transforms.Resize((self.input_size, self.input_size), Image.Resampling.BILINEAR)(img)\n",
    "            # img = transforms.RandomResizedCrop(size=self.input_size,scale=(0.4, 0.75),ratio=(0.5,1.5))(img)\n",
    "            # img = transforms.RandomCrop(self.input_size)(img)\n",
    "            img = transforms.RandomHorizontalFlip()(img)\n",
    "            img = transforms.ColorJitter(brightness=0.2, contrast=0.2)(img)\n",
    "\n",
    "            img = transforms.ToTensor()(img)\n",
    "            img = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(img)\n",
    "\n",
    "        else:\n",
    "            img, target = imageio.imread(self.test_img_label[index][0]), self.test_img_label[index][1]\n",
    "            if len(img.shape) == 2:\n",
    "                img = np.stack([img] * 3, 2)\n",
    "            img = Image.fromarray(img, mode='RGB')\n",
    "            img = transforms.Resize((self.input_size, self.input_size), Image.BILINEAR)(img)\n",
    "            # img = transforms.CenterCrop(self.input_size)(img)\n",
    "            img = transforms.ToTensor()(img)\n",
    "            img = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(img)\n",
    "\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.is_train:\n",
    "            return len(self.train_img_label)\n",
    "        else:\n",
    "            return len(self.test_img_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8c41e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=TUMOR(input_size, root, is_train=True, data_len=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "19d1b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train_label_file:\n",
    "#     print(os.path.join(train_img_path, line[:-2]), int(line[-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8929f86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test=torch.rand(1, 3, 448, 448)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e00a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d=model.pretrained_model_B(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cc176589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CAR'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d54b4a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainloader, testloader = read_dataset(input_size, batch_size, root, set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "45c09f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(trainloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fbf649ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MainNet(proposalN=proposalN, num_classes=num_classes, channels=2048, theta=(0.6), red_p=0.15)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "64d75b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainloader, testloader = read_dataset(input_size, batch_size, root, set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "832160d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed_x, _ = next(iter(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0b10f8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display image and label.\n",
    "# train_features, train_labels = next(iter(trainloader))\n",
    "# print(f\"Feature batch shape: {train_features.size()}\")\n",
    "# print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "# img = train_features[0].squeeze()\n",
    "# label = train_labels[0]\n",
    "# # plt.imshow(img, cmap=\"gray\")\n",
    "# # plt.show()\n",
    "# print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1a37e4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132f52b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "04d3432b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading car trainset\n",
      "Loading car testset\n",
      "Load model from ./checkpoint/car\\fm1fm2attention_50_resnet\\epoch160.pth\n",
      "Resume from ./checkpoint/car\\fm1fm2attention_50_resnet\\epoch160.pth\n",
      "Training 161 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1358/1358 [10:04<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1341/1341 [02:06<00:00, 10.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: raw accuracy: 93.81%, local accuracy: 94.45%, PCP: 0.00%\n",
      "Saving checkpoint\n",
      "Training 162 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1358/1358 [10:00<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1341/1341 [02:06<00:00, 10.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: raw accuracy: 93.51%, local accuracy: 94.13%, PCP: 0.00%\n",
      "Saving checkpoint\n",
      "Training 163 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1358/1358 [09:58<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1341/1341 [02:05<00:00, 10.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: raw accuracy: 93.74%, local accuracy: 94.20%, PCP: 0.00%\n",
      "Saving checkpoint\n",
      "Training 164 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1358/1358 [10:01<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1341/1341 [02:06<00:00, 10.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: raw accuracy: 93.69%, local accuracy: 94.15%, PCP: 0.00%\n",
      "Saving checkpoint\n",
      "Training 165 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|████████████                                                                   | 208/1358 [01:46<09:47,  1.96it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [65]\u001b[0m, in \u001b[0;36m<cell line: 70>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     58\u001b[0m     train(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     59\u001b[0m           trainloader\u001b[38;5;241m=\u001b[39mtrainloader,\n\u001b[0;32m     60\u001b[0m           testloader\u001b[38;5;241m=\u001b[39mtestloader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m           num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[0;32m     68\u001b[0m           save_interval\u001b[38;5;241m=\u001b[39msave_interval)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 71\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [65]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m time_str \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m shutil\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./config.py\u001b[39m\u001b[38;5;124m'\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124mconfig.py\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(time_str)))\n\u001b[1;32m---> 58\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m      \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m      \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m      \u001b[49m\u001b[43mend_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m      \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_interval\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, trainloader, testloader, criterion, optimizer, scheduler, save_path, start_epoch, end_epoch, num_epochs, save_interval)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m         total_loss \u001b[38;5;241m=\u001b[39m raw_loss \u001b[38;5;241m+\u001b[39m local_loss \u001b[38;5;241m+\u001b[39m windowscls_loss\n\u001b[1;32m---> 61\u001b[0m     \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     65\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en_2\\lib\\site-packages\\torch\\_tensor.py:473\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    465\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    466\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    471\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    472\u001b[0m     )\n\u001b[1;32m--> 473\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en_2\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#coding=utf-8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import shutil\n",
    "import time\n",
    "from config import num_classes, model_name, model_path, lr_milestones, lr_decay_rate, input_size, \\\n",
    "    root, end_epoch, save_interval, init_lr, batch_size, CUDA_VISIBLE_DEVICES, weight_decay, \\\n",
    "    proposalN, set, channels\n",
    "# from utils.train_model import train\n",
    "from utils.read_dataset import read_dataset\n",
    "from utils.auto_laod_resume import auto_load_resume\n",
    "# from networks.model import MainNet\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def main():\n",
    "    batch_size = 6\n",
    "    end_epoch = 205\n",
    "    input_size = 448\n",
    "#     set='MURA'\n",
    "#     root = './datasets/MURA_DATA'\n",
    "    model_name='fm1fm2attention_50_resnet'\n",
    "    num_epochs=30\n",
    "    trainloader, testloader = read_dataset(input_size, batch_size, root, set)\n",
    "#     batch = next(iter(trainloader))\n",
    "    \n",
    "    model = MainNet(proposalN=proposalN, num_classes=num_classes, channels=2048, theta=(0.6), red_p=0.15)\n",
    "\n",
    "\n",
    "    criterion = FocalLoss()\n",
    "\n",
    "    parameters = model.parameters()\n",
    "\n",
    "   \n",
    "    save_path = os.path.join(model_path, model_name)\n",
    "    if os.path.exists(save_path):\n",
    "        start_epoch, lr = auto_load_resume(model, save_path, status='train')\n",
    "        assert start_epoch < end_epoch\n",
    "    else:\n",
    "        os.makedirs(save_path)\n",
    "        start_epoch = 0\n",
    "        lr = init_lr\n",
    "\n",
    "    # define optimizers\n",
    "    optimizer = torch.optim.SGD(parameters, lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "\n",
    "    model = model.cuda()\n",
    "\n",
    "    scheduler = MultiStepLR(optimizer, milestones=lr_milestones, gamma=lr_decay_rate)\n",
    "#     make_dot(yhat, params=dict(list(model.named_parameters()))).render(\"MainModel_torchviz\", format=\"png\")\n",
    "\n",
    "    time_str = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    shutil.copy('./config.py', os.path.join(save_path, \"{}config.py\".format(time_str)))\n",
    "\n",
    "    train(model=model,\n",
    "          trainloader=trainloader,\n",
    "          testloader=testloader,\n",
    "          criterion=criterion,\n",
    "          optimizer=optimizer,\n",
    "          scheduler=scheduler,\n",
    "          save_path=save_path,\n",
    "          start_epoch=start_epoch,\n",
    "          end_epoch=end_epoch,\n",
    "          num_epochs=30,\n",
    "          save_interval=save_interval)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a0dd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(model_path, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02bfbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import shutil\n",
    "import time\n",
    "from config import num_classes, model_name, model_path, lr_milestones, lr_decay_rate, input_size, \\\n",
    "    root, end_epoch, save_interval, init_lr, batch_size, CUDA_VISIBLE_DEVICES, weight_decay, \\\n",
    "    proposalN, set, channels\n",
    "# from utils.train_model import train\n",
    "from utils.read_dataset import read_dataset\n",
    "from utils.auto_laod_resume import auto_load_resume\n",
    "# from networks.model import MainNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1836b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "def test_model(model, dataloaders, device):\n",
    "    CM=0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in dataloaders['test']:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images) #file_name\n",
    "            preds = torch.argmax(outputs.data, 1)\n",
    "            CM + = confusion_matrix(labels.cpu(), preds.cpu(),labels=[0,1])\n",
    "            \n",
    "        tn=CM[0][0]\n",
    "        tp=CM[1][1]\n",
    "        fp=CM[0][1]\n",
    "        fn=CM[1][0]\n",
    "        acc=np.sum(np.diag(CM)/np.sum(CM))\n",
    "        sensitivity=tp/(tp+fn)\n",
    "        precision=tp/(tp+fp)\n",
    "        \n",
    "        print('\\nTestset Accuracy(mean): %f %%' % (100 * acc))\n",
    "        print()\n",
    "        print('Confusion Matirx : ')\n",
    "        print(CM)\n",
    "        print('- Sensitivity : ',(tp/(tp+fn))*100)\n",
    "        print('- Specificity : ',(tn/(tn+fp))*100)\n",
    "        print('- Precision: ',(tp/(tp+fp))*100)\n",
    "        print('- NPV: ',(tn/(tn+fn))*100)\n",
    "        print('- F1 : ',((2*sensitivity*precision)/(sensitivity+precision))*100)\n",
    "        print()\n",
    "                \n",
    "    return acc, CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e0852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MainNet(proposalN=proposalN, num_classes=num_classes, channels=2048, theta=(0.6), red_p=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63db0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='one'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62870fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, testloader = read_dataset(input_size, batch_size, root, set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529bd801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc0794",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(model_path, model_name)\n",
    "if os.path.exists(save_path):\n",
    "    start_epoch, lr = auto_load_resume(model, save_path, status='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faafde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = {['train', 'test'][x]: [len(trainloader.dataset),len(testloader.dataset)][x] for x in range(len(['train', 'val']))}\n",
    "dataloaders = {['train', 'test'][x]: [trainloader,testloader][x] for x in range(len(['train', 'val']))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac39b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2b715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1af1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from config import input_size, root, proposalN, channels\n",
    "from utils.read_dataset import read_dataset\n",
    "from utils.auto_laod_resume import auto_load_resume\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if CUDA else \"cpu\")\n",
    "\n",
    "#load dataset\n",
    "_, testloader = read_dataset(input_size, batch_size, root, set)\n",
    "\n",
    "\n",
    "model = MainNet(proposalN=proposalN, num_classes=num_classes, channels=2048, theta=(0.6), red_p=0.15)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "if os.path.exists(save_path):\n",
    "    epoch = auto_load_resume(model, save_path, status='train')\n",
    "else:\n",
    "    sys.exit('There is not a pth exist.')\n",
    "\n",
    "print('Testing')\n",
    "raw_correct = 0\n",
    "object_correct = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(tqdm(testloader)):\n",
    "        if set == 'CUB':\n",
    "            x, y, boxes, _ = data\n",
    "        else:\n",
    "            x, y = data\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        local_logits, local_imgs = model(x, epoch, i, 'train', DEVICE)[-3:-1]\n",
    "        # local\n",
    "        pred = local_logits.max(1, keepdim=True)[1]\n",
    "        object_correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    print('\\nObject branch accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            object_correct, len(testloader.dataset), 100. * object_correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a22bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd04479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445a296d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7381cc37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542a254e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41c124b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
